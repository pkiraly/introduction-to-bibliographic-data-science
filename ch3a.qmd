# data acquisition {#sec-ch3a}

abstract: Main types of bibliographic data and data sources (library catalogs, citation databases, research data repositories, historical sources). Methods of data acquisition (standards, application programming interfaces, and tools), information about data structure (metadata schemas and serialization formats), terms of use.

Digital data can be retrieved in three main methods. The most convenient option is for these to be available as downloadable files (e.g., as reusable research data), however this type of data sharing is relatively rare. It is more common for data sources to be accessed through some kind of application programming interface. Various applications are available for the most common interfaces (OAI-PMH, Z39.50, SRW/SRU, SPARQL), so programming is not necessarily required, but time must be set aside to study the institution-specific settings and parameters of the interfaces. Finally, it is often the case that no previous opportunity was available. At this point, we extract the data from the HTML source of the website, assuming that the typographical formatting consistently indicates certain semantic elements17 – but in this case, it is worth consulting with the website operator to see if there are any other options not documented on the site. Whichever solution you choose, make sure that the data license allows reuse. After downloading the data, the first step is to import it. Programming libraries supporting various bibliographic formats are available; for MARC21, for example, there are ones for Java, Python, Go, JavaScript, R, PHP, and other programming languages. In this textbook we will work with Python, but the code could be adapted to other programming languages.

There are four main types of data sources for library history research: 

1. library catalogs (e.g., national libraries or general purpose union catalogs, as well as catalogs of specifically old books, such as the VD16, VD17 and VD18 series that register 16th, 17th, and 18th century German books, their Italian counterpart EDIT16, and the Heritage of the Printed Book database), 
2. digital library catalogs (Europeana, Gallica, German Digital Library, Hungarian Electronic Library, HathiTrust), 
3. citation databases and research data repositories (DataCite, Zenodo, OpenAlex, Open Citation), and finally 
4. databases of digitized (book) historical sources (the database of the Société Typographique de Neuchâtel, the database of 18th-century Dutch auction book catalogs MEDIATE, or the no defunct Eruditio in Hungary).

## Creating directories

In this textbook we will work with different directories:

- `data`: the data we created
- `raw-data`: our original data sources downloaded from data providers' sites
- `plots`: the output of data visualization

The first programming task is to create these directories. To create them we use the `os` Python library that provides standard operating system functions, such as `makedirs` that creates a directory.

```python
import os # <1>

directories = ['data', 'raw-data', 'plots'] # <2>

for directory in directories:  # <3>
    if not os.path.exists(directory): # <4>
        os.makedirs(directory) # <5>
```
1. import the `os` library
2. create a list with three strings, the names of the directories
3. iterate of the directories one by one. In each iteration the name of the current
directory will be stored in the variable `directory`
4. check if the directory does not exist. If it exists the script skips the directory and takes the next one. If it doesn't exist, it continue with the next command
5. creating the directory (due to the test in the previous line only if it doesn't exist already)

The code of this section is available at `scripts/ch3a-create-directories.py` of the repository.

## Download a MARCXML file

You can find a list of downloadable library catalogues [ḣere](https://github.com/pkiraly/qa-catalogue#datasources). 
Now download a relatively small file with MARC records of the Latvian National Bibliography (2014-2023) provided
by the [Open Data Portal](https://dati.lnb.lv/) of the National Library of Latvia. 
The files are compressed with zip, so in order to use them we should extract them, and
because the zip file contains a `data` subdirectory, we rename that to `lnb` (after the domain name of the National Library of Latvia).

```python
import urllib.request                         # <1>
import zipfile                                # <1>
import os                                     # <1>

url = 'https://dati.lnb.lv/files/natl_bibliography-2014-2023-marc.zip' # <2>
target_dir = 'raw-data'                                                # <2>
target_file = target_dir + '/lnb-natl_bibliography-2014-2023-marc.zip' # <2>

urllib.request.urlretrieve(url, target_file)  # <3>

with zipfile.ZipFile(target_file, 'r') as zip_ref: # <4>
    zip_ref.extractall(target_dir)

os.rename(target_dir + '/data', target_dir + '/lnb') # <5>
```

1. Import the Python libraries: `urllib` for the download, and `zipfile` for uncompressing
2. creating variables
3. call the download function with two arguments: the URL of the data source and the target file in our system
4. uncompress the zip file to our target directory
5. rename the `data` subdirectory (comes inside the zip file) to `lnb`

At the end of the process we will have a file `raw-data/lnb/natl_bibliography-2014-2023-marc.xml`, a set
of MARC21 record in MARCXML serialization format, that is MARC21 in XML. 

The code of this section is available at `scripts/ch3a-download-a-file.py` of the repository.

## Download an SQL file

Some bibliographic data sources are available as a relational database. These databases are
distributed as so called `SQL dump`, a non binary text based file, that contains both the
definition of the database structure (the properties of tables and columns), and the values. 
The dump files can be imported to the database. In this section we will import the following
database:

> Simon Burrows and Mark Curran, The French Book Trade in Enlightenment Europe Database, 
> 1769-1794 ([https://fbtee.westernsydney.edu.au/stn/interface/](https://fbtee.westernsydney.edu.au/stn/interface/), 6 May 2014)

It maps the trade of the Société Typographique de Neuchâtel, 1769-1794, and based on the almost 
intactly survided archive of the alliance of the printers at Neuchâtel (Switzerland). The database 
(henceforth _FBTEE_) could be imported into MySQL or its Open Source form MariaDB. the
database is downloadable from
[https://fbtee.westernsydney.edu.au/main/download-database/](https://fbtee.westernsydney.edu.au/main/download-database/).

As now we would like to download another file we might choose to copy the previous script, and replace the
variables `url` and `target_file` with another one, but it would duplicate the usiness logic of the script.
In software development there is a principle _Don't Repeat Yourself_  abbreviated as _DRY_. It suggest
that we should avoid from code repetition, or other words: the code should be reusable, Instead of writing
two (or more) specialized scripts that repeat the actual download and uncompressin part, we will create
a function, that will be used by these scripts.

First we will create a utility script that will contain functions we can use in the course. It will
be called bdsutils (short for Bibliographic Data Science utilities). This is the 

```python
import urllib.request
import zipfile

target_dir = 'raw-data'                     # <1>

def download_and_unzip(url, target_file):   # <2>
    """                                     # <3>
    Download a zip file, and uncompress it  # <3>
    
    Parameters                              # <3>
    ----------                              # <3>
    url : str                               # <3>
        The URL of the zip file to download # <3>
    sound : str                             # <3>
        The local file name                 # <3>
    """                                     # <3>

    urllib.request.urlretrieve(url, target_file)

    with zipfile.ZipFile(target_file, 'r') as zip_ref:
        zip_ref.extractall(target_dir)
```

1. the script keeps only one variable `target_dir`, that will remane the same accross calls
2. the definition of the method
3. the documentation of the method

:::{.callout-important}
TODO: The code should be explained more!
:::

The file is available in the repository as `scripts/bdsutils.py`.

Once we have this utility script, we can create the specific one for _FBTEE_. This zip file 
does not contain directories, only a single file _chop_leeds_ac_uk.sql_. 

```python
from bdsutils import target_dir, download_and_unzip # <1>
import os

url = 'https://fbtee.uws.edu.au/stn/database/download/STN_database.zip' # <2>
target_file = target_dir + '/stn_database.zip'

download_and_unzip(url, target_file) # <3>

directory = target_dir + '/fbtee'
if not os.path.exists(directory): # <4>
    os.mkdir(directory)

os.rename(target_dir + '/chop_leeds_ac_uk.sql', # <5>
          directory + '/chop_leeds_ac_uk.sql')
```

1. import both the variable and the function from bdsutils (we should not add the `.py` extension). 
   This way we should not use the prefix bdsutils for these components (such as `bdsutils.target_dir`)
2. the definition of the _FBTEE_ specific variables
3. usage of the method defined in bdsutils
4. create a dedicated directory for the database if it doesn't already exist
5. move the file to this directory

If you run it, you will have two files:
- raw-data/fbtee/chop_leeds_ac_uk.sql
- raw-data/stn_database.zip

The file is available in the repository as `scripts/ch3a-download-stn.py`.

### import BTEE to the database

Importing a datadump into MySQL or MariaDB is a two step process. First you should create the database 
itself with permissions (for security reason it is advisable to create a dedicated user as well and 
not to use the default database user), then you can import the data.

Step 1. Prepare the database

Log in to MySQL, then create the database `fbtee` and a dedicated user (replace `<username>` and `<password>`
with your choosen credentials):

```SQL
CREATE DATABASE fbtee;

CREATE USER '<username>'@'localhost' IDENTIFIED BY '<password>';
GRANT ALL PRIVILEGES ON fbtee.* TO '<username>'@'localhost' WITH GRANT OPTION;
FLUSH PRIVILEGES;
quit
```

Note: you can choose any name for the database, this material will use `fbtee`.

Step 2. Import the data dump

```bash
mysql -u <username> -p fbtee < raw-data/fbtee/chop_leeds_ac_uk.sql
```

This command will ask for the password. Use the same credentials you set in Step 1. If you did not 
get any error message, you can check the database structure. Log in again with the
dedicated credentials:

```bash
mysql -u <username> -p fbtee
```

The you can check the list of tables with

```SQL
SHOW TABLES;
```

that returns
```
+-------------------------------------+
| Tables_in_fbtee                     |
+-------------------------------------+
| authors                             |
| books                               |
| books_authors                       |
| books_call_numbers                  |
| books_stn_catalogues                |
| clients                             |
| clients_addresses                   |
| clients_correspondence_manuscripts  |
| clients_correspondence_places       |
| clients_people                      |
| clients_professions                 |
| keyword_assignments                 |
| keyword_free_associations           |
| keyword_tree_associations           |
| keywords                            |
| orders                              |
| orders_agents                       |
| orders_sent_via                     |
| orders_sent_via_place               |
| parisian_keywords                   |
| parisian_system_keyword_assignments |
| people                              |
| people_professions                  |
| places                              |
| professions                         |
| super_books                         |
| super_books_keywords                |
| tags                                |
| transactions                        |
| transactions_volumes_exchanged      |
+-------------------------------------+
30 rows in set (0,001 sec)
```

or the structure of the `books` table with

```SQL
DESCRIBE books;
```

The result:

```
+---------------------------+---------------+------+-----+---------+-------+
| Field                     | Type          | Null | Key | Default | Extra |
+---------------------------+---------------+------+-----+---------+-------+
| book_code                 | char(9)       | NO   | PRI | NULL    |       |
| super_book_code           | char(11)      | NO   |     | NULL    |       |
| edition_status            | varchar(15)   | NO   |     | NULL    |       |
| edition_type              | varchar(50)   | NO   |     | NULL    |       |
| full_book_title           | varchar(750)  | YES  | MUL | NULL    |       |
| short_book_titles         | varchar(1000) | YES  |     | NULL    |       |
| translated_title          | varchar(750)  | YES  |     | NULL    |       |
| translated_language       | varchar(50)   | YES  | MUL | NULL    |       |
| languages                 | varchar(200)  | YES  | MUL | NULL    |       |
| stated_publishers         | varchar(1000) | YES  | MUL | NULL    |       |
| actual_publishers         | varchar(1000) | YES  |     | NULL    |       |
| stated_publication_places | varchar(1000) | YES  |     | NULL    |       |
| actual_publication_places | varchar(1000) | YES  |     | NULL    |       |
| stated_publication_years  | varchar(1000) | YES  |     | NULL    |       |
| actual_publication_years  | varchar(10)   | YES  |     | NULL    |       |
| pages                     | varchar(250)  | YES  |     | NULL    |       |
| quick_pages               | varchar(10)   | YES  |     | NULL    |       |
| number_of_volumes         | int(11)       | YES  |     | NULL    |       |
| section                   | varchar(10)   | YES  |     | NULL    |       |
| edition                   | varchar(100)  | YES  |     | NULL    |       |
| book_sheets               | varchar(200)  | YES  |     | NULL    |       |
| notes                     | varchar(4000) | YES  |     | NULL    |       |
| research_notes            | varchar(1000) | YES  |     | NULL    |       |
+---------------------------+---------------+------+-----+---------+-------+
23 rows in set (0,001 sec)
```

This material is not intend to provide introduction to SQL, if you are interested, we 
can suggest the Library Carpentry's [SQL lesson](https://librarycarpentry.github.io/lc-sql/). 