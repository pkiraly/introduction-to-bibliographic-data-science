# data acquisition {#sec-ch3a}

abstract: Main types of bibliographic data and data sources (library catalogs, citation databases, research data repositories, historical sources). Methods of data acquisition (standards, application programming interfaces, and tools), information about data structure (metadata schemas and serialization formats), terms of use.

Digital data can be retrieved in three main methods. The most convenient option is for these to be available as downloadable files (e.g., as reusable research data), however this type of data sharing is relatively rare. It is more common for data sources to be accessed through some kind of application programming interface. Various applications are available for the most common interfaces (OAI-PMH, Z39.50, SRW/SRU, SPARQL), so programming is not necessarily required, but time must be set aside to study the institution-specific settings and parameters of the interfaces. Finally, it is often the case that no previous opportunity was available. At this point, we extract the data from the HTML source of the website, assuming that the typographical formatting consistently indicates certain semantic elements17 – but in this case, it is worth consulting with the website operator to see if there are any other options not documented on the site. Whichever solution you choose, make sure that the data license allows reuse. After downloading the data, the first step is to import it. Programming libraries supporting various bibliographic formats are available; for MARC21, for example, there are ones for Java, Python, Go, JavaScript, R, PHP, and other programming languages. In this textbook we will work with Python, but the code could be adapted to other programming languages.

There are four main types of data sources for library history research: 

1. library catalogs (e.g., national libraries or general purpose union catalogs, as well as catalogs of specifically old books, such as the VD16, VD17 and VD18 series that register 16th, 17th, and 18th century German books, their Italian counterpart EDIT16, and the Heritage of the Printed Book database), 
2. digital library catalogs (Europeana, Gallica, German Digital Library, Hungarian Electronic Library, HathiTrust), 
3. citation databases and research data repositories (DataCite, Zenodo, OpenAlex, Open Citation), and finally 
4. databases of digitized (book) historical sources (the database of the Société Typographique de Neuchâtel, the database of 18th-century Dutch auction book catalogs MEDIATE, or the no defunct Eruditio in Hungary).

## Creating directories

In this textbook we will work with different directories:

- `data`: the data we created
- `raw-data`: our original data sources downloaded from data providers' sites
- `plots`: the output of data visualization

The first programming task is to create these directories. To create them we use the `os` Python library that provides standard operating system functions, such as `makedirs` that creates a directory.

```python
import os # <1>

directories = ['data', 'raw-data', 'plots'] # <2>

for directory in directories:  # <3>
    if not os.path.exists(directory): # <4>
        os.makedirs(directory) # <5>
```
1. import the `os` library
2. create a list with three strings, the names of the directories
3. iterate of the directories one by one. In each iteration the name of the current
directory will be stored in the variable `directory`
4. check if the directory does not exist. If it exists the script skips the directory and takes the next one. If it doesn't exist, it continue with the next command
5. creating the directory (due to the test in the previous line only if it doesn't exist already)

The code of this section is available at `scripts/ch3a-create-directories.py` of the repository.

## Download a MARCXML file

You can find a list of downloadable library catalogues [ḣere](https://github.com/pkiraly/qa-catalogue#datasources). 
Now download a relatively small file with MARC records of the Latvian National Bibliography (2014-2023) provided
by the [Open Data Portal](https://dati.lnb.lv/) of the National Library of Latvia. 
The files are compressed with zip, so in order to use them we should extract them, and
because the zip file contains a `data` subdirectory, we rename that to `lnb` (after the domain name of the National Library of Latvia).

```python
import urllib.request                         # <1>
import zipfile                                # <1>
import os                                     # <1>

url = 'https://dati.lnb.lv/files/natl_bibliography-2014-2023-marc.zip' # <2>
target_dir = 'raw-data'                                                # <2>
target_file = target_dir + '/lnb-natl_bibliography-2014-2023-marc.zip' # <2>

urllib.request.urlretrieve(url, target_file)  # <3>

with zipfile.ZipFile(target_file, 'r') as zip_ref: # <4>
    zip_ref.extractall(target_dir)

os.rename(target_dir + '/data', target_dir + '/lnb') # <5>
```

1. Import the Python libraries: `urllib` for the download, and `zipfile` for uncompressing
2. creating variables
3. call the download function with two arguments: the URL of the data source and the target file in our system
4. uncompress the zip file to our target directory
5. rename the `data` subdirectory (comes inside the zip file) to `lnb`

At the end of the process we will have a file `raw-data/lnb/natl_bibliography-2014-2023-marc.xml`, a set
of MARC21 record in MARCXML serialization format, that is MARC21 in XML. 

The code of this section is available at `scripts/ch3a-download-a-file.py` of the repository.

## APIs to retrieve records

### OAI-PMH

> The Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) is a 
> low-barrier mechanism for repository interoperability. Data Providers are repositories 
> that expose structured metadata via OAI-PMH. Service Providers then make OAI-PMH 
> service requests to harvest that metadata. OAI-PMH is a set of six verbs or services 
> that are invoked within HTTP.
> From [https://www.openarchives.org/pmh/](https://www.openarchives.org/pmh/)

The protocol provides 6 'verbs' or endpoints to retreive information:

* [GetRecord](https://www.openarchives.org/OAI/openarchivesprotocol.html#GetRecord): to retrieve an individual metadata record
* [Identify](https://www.openarchives.org/OAI/openarchivesprotocol.html#Identify): information about a repository
* [ListIdentifiers](https://www.openarchives.org/OAI/openarchivesprotocol.html#ListIdentifiers): to harvest record identifiers
* [ListMetadataFormats](https://www.openarchives.org/OAI/openarchivesprotocol.html#ListMetadataFormats): to retrieve the metadata formats available from a repository. These formats are alternative XML representations of the same content, but it also happens that a library provides different content via different formats. In the `ListIdentifiers` and `ListRecords` verbs it is used as the `metadataPrefix` parameter. At least one format, the Dublin Core is mandatory. 
* [ListRecords](https://www.openarchives.org/OAI/openarchivesprotocol.html#ListRecords): to harvest records
* [ListSets](https://www.openarchives.org/OAI/openarchivesprotocol.html#ListSets): to retrieve the set structure of a repository. Sets are optional collections within a repository, e.g. there might be individual sets according to the physical collections of a library. 

The response of these verbs is always XML. For the List* verbs the maintainer of the service 
sets a pager size, so you do not retrieve all records at once only limeted number (e.g. 100).
The pagination is implemented by a distinct XML element called `resumptionToken`, which the
client should use in the next call. Its value can change from call to call, so the client always
have to extract is. In the above links you can find more detailed information and examples.

There are different implementations of a OAI-PMH clients in Python, we use 
[Mathias Loesch](https://github.com/mloesch)'s [Sickle](https://github.com/mloesch/sickle) 
module (version 0.7.0) to retrieve the Estonian National Bibliography from the 
[DataLab](https://digilab.rara.ee/en/), the data sharing platform of the Estonian National Library.
At the time of writing it returns somewhat almost 410 000 MARCXML records. We save them
into local files each containing 100 000 records.
As OAI-PMH is based on XML technology we use [lxml](https://lxml.de/) 
module (version 6.0.0), a lightweight XML and HTML processing toolbox.

```python
import io
import sys
import os
from lxml import etree
from sickle import Sickle

if len(sys.argv) == 1:                                  # <1>
    print('Please give a directory name')
    exit()
dir = sys.argv[1]                                       # <2>
if not os.path.exists(dir):
    os.mkdir(dir)
print('saving to %s' % (dir))

namespaces = {                                          # <3>
    'oai': 'http://www.openarchives.org/OAI/2.0/',
    'marc21': 'http://www.loc.gov/MARC21/slim'
}

header = '<?xml version="1.0" encoding="utf8"?>' + "\n" \ # <4>
       + '<collection>' + "\n"
footer = '</collection>'

def write_output(xmlrecords, file_counter, dir_name):           # <5>
    """
    Writes MARC records to file
    Parameters                              
    ----------
    xmlrecords : list
        A list of string contains the individual MARCXML records
    file_counter : int
        A file name counter
    dir_name : int
        The name of the output directory
    """
    file_name = "%s/%06d.xml" % (dir_name, counter)
    print(file_name)
    with io.open(file_name, 'w', encoding='utf8') as f:
        f.write(header)
        f.write("\n".join(xmlrecords) + "\n")
        f.write(footer)

sickle = Sickle('https://data.digar.ee/repox/OAIHandler', max_retries=4) # <6>

xmlrecords = []                                                  # <7>
file_counter = 0
record_counter = 0

it = sickle.ListRecords(metadataPrefix='marc21xml', set='erb')  # <8>
for content in it:                                              # <9>
    tree = etree.ElementTree(content)                           # <10>      

    records = tree.xpath(
                  '/oai:record[*]/oai:metadata/marc21:record',  # <11>
                  namespaces=namespaces)
    for record in records:                                      # <12>
        xmlrecord = etree\                                      # <13>
              .tostring(record, encoding='utf8', method='xml')\ # <14>
              .decode("utf-8")\                                 # <15>
              .replace("<?xml version='1.0' encoding='utf8'?>\n", '') # <16>
        xmlrecords.append(xmlrecord)                            # <17>
        record_counter += 1                                     # <18>

    if len(xmlrecords) >= 100000:                               # <19>
        write_output(xmlrecords, file_counter, dir)             # <20>
        xmlrecords = []                                         # <21>
        file_counter += 1

if len(xmlrecords) > 0:                                        # <22>
    write_output(xmlrecords, file_counter, dir)
print('saved %d records to %d files' % (record_counter, file_counter)) # <23>
```

1. When we run Python `sys.argv` list contains the arguments we passed to the interpreter. 
   The `len()` function returns the number of elements in a collection, so it gives here
   the number of arguments. The first argument is the name of the currect script, so if the
   number here is zero it means that the script itself does not have any argument. We should
   pass at least one argument: the name of the directory where the script will store the
   records - if we don't provide it, the script will send us a message and stops running
   (with the `exit()` function).
2. If we provide an argument it will be stored as the name of the output directory. Then 
   it creates the directory if it not already exists, and notify us.
3. `namespaces` variable registers the XML namespaces the process needs - otherwise the
   XPath expresssions would not work.
4. `header` and `footer` needs at the beginning and end of the output XML files. The XML 
   files are hierarchical: their main content is a list of MARC records, and their parent
   element will be the `<collection>`.
5. `write_output()` function creates the output files in the specified directory. It 
   prints the XML header, join together and prints the XML records, and finally prints the
   XML footer.
6. The Sickle harvester is initialized with the endpoint of the OAI-PMH service and an extra
   argument to specify how many times it can retry to fetch a single URL. It is needed because
   sometimes there are communication problems between the server and the client (due to
   problems on either side or in the network). 
7. Initialisation of the necessary variables: the collector of the individual records,
   the output file counter and the record counter.
8. Start the harvest using the `ListRecords` verb. It returns an iterator, so we should
   not take care of pagination with individual HTTP calls and resumptionToken mentioned above,
   it is handled automatically by the Sickle module. We use two arguments: we set the 
   metadata schema as MARCXML and the set as `erb` which stands for the Estonian National Bibliography.
   These values can be extracted from a preliminary investigation of the supported values
   returned by the `ListMetadataFormats` and `ListSets` verbs.
9. In the following the script iterates over the individual XML responses of the calls.
10. The raw XML content is transormed to an [XML element tree](https://docs.python.org/3/library/xml.etree.elementtree.html)
    that is an internal data structure with an API.
11. Selection of the individual records with an XPath expression. For the expression
    we should provide the XML namespace. In OAI-PMH the individual records (`<oai:record>`)
    have two parts: a header, that contains record identifier and other metadata (such as the
    set it belongs to, the date of last modification) and the actual record (`<oai:metadata>`).
    This later is the container of the MARCXML record (`<marc21:record>`). If you would like to
    apply this script to another OAI-PMH server, please check the namespaces it uses. The 
    abbreviations are not important, you can name them as you like (however to keep them as in
    the source, i.e. the XML response of the service helps in debugging the problems, if there are),
    but you should always use the URLs used by the service.
12. Iterate over the indifidual records that `xpath()` returned.
13. Transformation of the above mentioned tree structure of the record to XML via a sequence of steps:
14. create the XML string...
15. convert to UTF-8 encoding...
16. remove the [XML declaration](https://www.w3.org/TR/REC-xml/#dt-xmldecl) - we do not need it for
    each records.
17. add the record into the record collection
18. increase the record counter by one
19. if the record collection contains more than 100 000 records...
20. the records should write to a file...
21. then empty the collection, and increase file counter by one
22. after the iteration if the record collection is not empty write it to a file
23. finally notify the user about the number of records ingested and files written 


## Download an SQL file

Some bibliographic data sources are available as a relational database. These databases are
distributed as so called `SQL dump`, a non binary text based file, that contains both the
definition of the database structure (the properties of tables and columns), and the values. 
The dump files can be imported to the database. In this section we will import the following
database:

> Simon Burrows and Mark Curran, The French Book Trade in Enlightenment Europe Database, 
> 1769-1794 ([https://fbtee.westernsydney.edu.au/stn/interface/](https://fbtee.westernsydney.edu.au/stn/interface/), 6 May 2014)

It maps the trade of the Société Typographique de Neuchâtel, 1769-1794, and based on the almost 
intactly survided archive of the alliance of the printers at Neuchâtel (Switzerland). The database 
(henceforth _FBTEE_) could be imported into MySQL or its Open Source form MariaDB. the
database is downloadable from
[https://fbtee.westernsydney.edu.au/main/download-database/](https://fbtee.westernsydney.edu.au/main/download-database/).

As now we would like to download another file we might choose to copy the previous script, and replace the
variables `url` and `target_file` with another one, but it would duplicate the usiness logic of the script.
In software development there is a principle _Don't Repeat Yourself_  abbreviated as _DRY_. It suggest
that we should avoid from code repetition, or other words: the code should be reusable, Instead of writing
two (or more) specialized scripts that repeat the actual download and uncompressin part, we will create
a function, that will be used by these scripts.

First we will create a utility script that will contain functions we can use in the course. It will
be called bdsutils (short for Bibliographic Data Science utilities). This is the 

```python
import urllib.request
import zipfile

target_dir = 'raw-data'                     # <1>

def download_and_unzip(url, target_file):   # <2>
    """                                     # <3>
    Download a zip file, and uncompress it  # <3>
    
    Parameters                              # <3>
    ----------                              # <3>
    url : str                               # <3>
        The URL of the zip file to download # <3>
    sound : str                             # <3>
        The local file name                 # <3>
    """                                     # <3>

    urllib.request.urlretrieve(url, target_file)

    with zipfile.ZipFile(target_file, 'r') as zip_ref:
        zip_ref.extractall(target_dir)
```

1. the script keeps only one variable `target_dir`, that will remane the same accross calls
2. the definition of the method
3. the documentation of the method

:::{.callout-important}
TODO: The code should be explained more!
:::

The file is available in the repository as `scripts/bdsutils.py`.

Once we have this utility script, we can create the specific one for _FBTEE_. This zip file 
does not contain directories, only a single file _chop_leeds_ac_uk.sql_. 

```python
from bdsutils import target_dir, download_and_unzip # <1>
import os

url = 'https://fbtee.uws.edu.au/stn/database/download/STN_database.zip' # <2>
target_file = target_dir + '/stn_database.zip'

download_and_unzip(url, target_file) # <3>

directory = target_dir + '/fbtee'
if not os.path.exists(directory): # <4>
    os.mkdir(directory)

os.rename(target_dir + '/chop_leeds_ac_uk.sql', # <5>
          directory + '/chop_leeds_ac_uk.sql')
```

1. import both the variable and the function from bdsutils (we should not add the `.py` extension). 
   This way we should not use the prefix bdsutils for these components (such as `bdsutils.target_dir`)
2. the definition of the _FBTEE_ specific variables
3. usage of the method defined in bdsutils
4. create a dedicated directory for the database if it doesn't already exist
5. move the file to this directory

If you run it, you will have two files:
- raw-data/fbtee/chop_leeds_ac_uk.sql
- raw-data/stn_database.zip

The file is available in the repository as `scripts/ch3a-download-stn.py`.

### import BTEE to the database

Importing a datadump into MySQL or MariaDB is a two step process. First you should create the database 
itself with permissions (for security reason it is advisable to create a dedicated user as well and 
not to use the default database user), then you can import the data.

Step 1. Prepare the database

Log in to MySQL, then create the database `fbtee` and a dedicated user (replace `<username>` and `<password>`
with your choosen credentials):

```SQL
CREATE DATABASE fbtee;

CREATE USER '<username>'@'localhost' IDENTIFIED BY '<password>';
GRANT ALL PRIVILEGES ON fbtee.* TO '<username>'@'localhost' WITH GRANT OPTION;
FLUSH PRIVILEGES;
quit
```

Note: you can choose any name for the database, this material will use `fbtee`.

Step 2. Import the data dump

```bash
mysql -u <username> -p fbtee < raw-data/fbtee/chop_leeds_ac_uk.sql
```

This command will ask for the password. Use the same credentials you set in Step 1. If you did not 
get any error message, you can check the database structure. Log in again with the
dedicated credentials:

```bash
mysql -u <username> -p fbtee
```

The you can check the list of tables with

```SQL
SHOW TABLES;
```

that returns
```
+-------------------------------------+
| Tables_in_fbtee                     |
+-------------------------------------+
| authors                             |
| books                               |
| books_authors                       |
| books_call_numbers                  |
| books_stn_catalogues                |
| clients                             |
| clients_addresses                   |
| clients_correspondence_manuscripts  |
| clients_correspondence_places       |
| clients_people                      |
| clients_professions                 |
| keyword_assignments                 |
| keyword_free_associations           |
| keyword_tree_associations           |
| keywords                            |
| orders                              |
| orders_agents                       |
| orders_sent_via                     |
| orders_sent_via_place               |
| parisian_keywords                   |
| parisian_system_keyword_assignments |
| people                              |
| people_professions                  |
| places                              |
| professions                         |
| super_books                         |
| super_books_keywords                |
| tags                                |
| transactions                        |
| transactions_volumes_exchanged      |
+-------------------------------------+
30 rows in set (0,001 sec)
```

or the structure of the `books` table with

```SQL
DESCRIBE books;
```

The result:

```
+---------------------------+---------------+------+-----+---------+-------+
| Field                     | Type          | Null | Key | Default | Extra |
+---------------------------+---------------+------+-----+---------+-------+
| book_code                 | char(9)       | NO   | PRI | NULL    |       |
| super_book_code           | char(11)      | NO   |     | NULL    |       |
| edition_status            | varchar(15)   | NO   |     | NULL    |       |
| edition_type              | varchar(50)   | NO   |     | NULL    |       |
| full_book_title           | varchar(750)  | YES  | MUL | NULL    |       |
| short_book_titles         | varchar(1000) | YES  |     | NULL    |       |
| translated_title          | varchar(750)  | YES  |     | NULL    |       |
| translated_language       | varchar(50)   | YES  | MUL | NULL    |       |
| languages                 | varchar(200)  | YES  | MUL | NULL    |       |
| stated_publishers         | varchar(1000) | YES  | MUL | NULL    |       |
| actual_publishers         | varchar(1000) | YES  |     | NULL    |       |
| stated_publication_places | varchar(1000) | YES  |     | NULL    |       |
| actual_publication_places | varchar(1000) | YES  |     | NULL    |       |
| stated_publication_years  | varchar(1000) | YES  |     | NULL    |       |
| actual_publication_years  | varchar(10)   | YES  |     | NULL    |       |
| pages                     | varchar(250)  | YES  |     | NULL    |       |
| quick_pages               | varchar(10)   | YES  |     | NULL    |       |
| number_of_volumes         | int(11)       | YES  |     | NULL    |       |
| section                   | varchar(10)   | YES  |     | NULL    |       |
| edition                   | varchar(100)  | YES  |     | NULL    |       |
| book_sheets               | varchar(200)  | YES  |     | NULL    |       |
| notes                     | varchar(4000) | YES  |     | NULL    |       |
| research_notes            | varchar(1000) | YES  |     | NULL    |       |
+---------------------------+---------------+------+-----+---------+-------+
23 rows in set (0,001 sec)
```

This material is not intend to provide introduction to SQL, if you are interested, we 
can suggest the Library Carpentry's [SQL lesson](https://librarycarpentry.github.io/lc-sql/). 