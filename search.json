[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bibliographic Data Science",
    "section": "",
    "text": "Preface\nThis book is an attempt to implement the ideas described in the essay An outline of an imagined training course on bibliographic data science published in Bibliographic data blog, 2026.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Preparation\nBibliographic data science is a relatively new interdisciplinary field of research that lies at the intersection of library science (or, more broadly, cultural heritage science), history and social sciences, and certain components of computer science. The objective of bibliographic data science is to establish previously hidden or possibly only suspected historical or collection trends based on data sources containing a (typically but not exclusively) large number of bibliographic records, ideally all those related to a given topic (e.g., national bibliographies), and on data science methods. Some of the field’s research questions:\nAlthough digital humanities education has developed dynamically in recent years, computer-based analysis of bibliographic sources is unfortunately rarely featured, and similarly absent from library science and IT education. In my opinion, this gap could be remedied by a new informal vocational training program that would appeal to those who are interested in some of the above issues and who already have some knowledge in one of the relevant fields (e.g., library science, cultural history, literary sociology, information technology). The analysis of records based on library bibliographic standards would probably also be of interest in library training. The training may take the form of a summer university or a seminar/course jointly organized by several university departments. Participants in the training could be university students or practicing professionals.\nIn this book we use the Python programming language. You should have a basic knowledge of the language and should know how to install it on your machine. In order to separate our environment from already installed Python modules, we use a virtual environment. To create it run the following:\nWhen you run the code in the book, you should first activate this virtual environment:\n… and when you finish the session, you should deactivate it:\nWhen we talk about installing a module you should do it within this environment, then you can use the standard Python module installation method:\nWe provide a list of modules used in this book, you can install them in a single step as:\nSome of the code examples run in the command line and written in bash, that is available by default in Linux and Mac machines. For Windows you can install it via WLS.\nWhen you enter this virtual Ubuntu the first time, you should give a user name (which might be the same or different as your Windows user name), and a password.\nYou can find more details and troubleshooting in the following documentation page: WSL Installation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#preparation",
    "href": "introduction.html#preparation",
    "title": "1  Introduction",
    "section": "",
    "text": "python -m venv venv\n\nsource venv/bin/activate\n\ndeactivate\n\nvenv/bin/pip install pandas\n\nvenv/bin/pip install -r requirements.txt\n\n\nOpen command line or PowerShell and enter:\n\nwsl --install -d Ubuntu\nwsl --set-default-version 2\nwsl --set-default ubuntu\n\nin Windows search enter Ubuntu and click on the Ubuntu icon, or in command line/PowerShell enter\n\nubuntu\n\n\n\n\n\n\nBourdieu, Pierre. 2008. “A Conservative Revolution in Publishing.” Translation Studies 1 (2): 123–53. https://doi.org/10.1080/14781700802113465.\n\n\nFarkas, Farkas Gábor, János Káldos, and Péter Király. 2025. “A Régi Magyarországi Kiadványok „Sötét Anyaga”.” Magyar Könyvszemle 141 (2): 226–66. https://doi.org/10.17167/mksz.2025.2.226-266.\n\n\nHeilbron, Johan. 1999. “Towards a Sociology of Translation: Book Translations as a Cultural World-System.” European Journal of Social Theory 2 (4): 429–44. https://doi.org/10.1177/136843199002004002.\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th Century? A Co-Occurrence Analysis of Early Modern German Dissertation Titles.” Synthese 203 (2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations in French Libraries: The EMDFL Dataset.” Journal of Open Humanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nKirály, Péter. 2019. “Measuring Metadata Quality.” Doctoral dissertation, Göttingen: University of Göttingen. https://doi.org/10.13140/RG.2.2.33177.77920.\n\n\nKirály, Péter, and András Kiséry. 2025. “‘Mór Jókai, Alas’: The Most Successful Hungarian Writer. A Quantitative Analysis.” Patterns of Translation. https://translationpatterns.substack.com/p/mor-jokai-alas-the-most-successful.\n\n\nKirály, Péter, Tomasz Umerle, Vojtěch Malínek, Elżbieta Herden, Beata Koper, Giovanni Colavizza, Rindert Jagersma, et al. 2025. “Effects of Open Science and the Digital Transformation on the Bibliographical Data Landscape.” In Library Catalogues as Data, edited by Paul Gooding, Melissa Terras, and Sarah Ames, 1st ed., 19–44. Facet. https://doi.org/10.29085/9781783306602.004.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019. “Bibliographic Data Science and the History of the Book (c. 1500–1800).” Cataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.\n\n\nSzemes, Botond, and Kata Dobás. 2025. “A Visegrádi Országok Digitális Irodalmi Emlékezete : Wikipedia, Wikidata – a Regionális Irodalomtörténet Új Alakzatai.” Irodalomtörténeti Közlemények 129 (2): 191–212. https://doi.org/10.56232/itk.2025.2.04.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch2a.html",
    "href": "ch2a.html",
    "title": "3  Scientific theories and models",
    "section": "",
    "text": "Darnton and his followers, Bourdieu, quantitative history, computational and data models of historical events – Thibodeau and Thaller",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scientific theories and models</span>"
    ]
  },
  {
    "objectID": "ch2b.html",
    "href": "ch2b.html",
    "title": "4  Cultural heritage data models",
    "section": "",
    "text": "the work-expression-manifestation-item model and its branches, ontologies, archival data models",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cultural heritage data models</span>"
    ]
  },
  {
    "objectID": "ch3a.html",
    "href": "ch3a.html",
    "title": "5  data acquisition",
    "section": "",
    "text": "5.1 Creating directories\nabstract: Main types of bibliographic data and data sources (library catalogs, citation databases, research data repositories, historical sources). Methods of data acquisition (standards, application programming interfaces, and tools), information about data structure (metadata schemas and serialization formats), terms of use.\nDigital data can be retrieved in three main methods. The most convenient option is for these to be available as downloadable files (e.g., as reusable research data), however this type of data sharing is relatively rare. It is more common for data sources to be accessed through some kind of application programming interface. Various applications are available for the most common interfaces (OAI-PMH, Z39.50, SRW/SRU, SPARQL), so programming is not necessarily required, but time must be set aside to study the institution-specific settings and parameters of the interfaces. Finally, it is often the case that no previous opportunity was available. At this point, we extract the data from the HTML source of the website, assuming that the typographical formatting consistently indicates certain semantic elements17 – but in this case, it is worth consulting with the website operator to see if there are any other options not documented on the site. Whichever solution you choose, make sure that the data license allows reuse. After downloading the data, the first step is to import it. Programming libraries supporting various bibliographic formats are available; for MARC21, for example, there are ones for Java, Python, Go, JavaScript, R, PHP, and other programming languages. In this textbook we will work with Python, but the code could be adapted to other programming languages.\nThere are four main types of data sources for library history research:\nIn this textbook we will work with different directories:\nThe first programming task is to create these directories. To create them we use the os Python library that provides standard operating system functions, such as makedirs that creates a directory.\nThe code of this section is available at scripts/ch3a-create-directories.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#creating-directories",
    "href": "ch3a.html#creating-directories",
    "title": "5  data acquisition",
    "section": "",
    "text": "data: the data we created\nraw-data: our original data sources downloaded from data providers’ sites\nplots: the output of data visualization\n\n\n1import os\n\n2directories = ['data', 'raw-data', 'plots']\n\n3for directory in directories:\n4    if not os.path.exists(directory):\n5        os.makedirs(directory)\n\n1\n\nimport the os library\n\n2\n\ncreate a list with three strings, the names of the directories\n\n3\n\niterate of the directories one by one. In each iteration the name of the current directory will be stored in the variable directory\n\n4\n\ncheck if the directory does not exist. If it exists the script skips the directory and takes the next one. If it doesn’t exist, it continue with the next command\n\n5\n\ncreating the directory (due to the test in the previous line only if it doesn’t exist already)",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#download-a-marcxml-file",
    "href": "ch3a.html#download-a-marcxml-file",
    "title": "5  data acquisition",
    "section": "5.2 Download a MARCXML file",
    "text": "5.2 Download a MARCXML file\nYou can find a list of downloadable library catalogues ḣere. Now download a relatively small file with MARC records of the Latvian National Bibliography (2014-2023) provided by the Open Data Portal of the National Library of Latvia. The files are compressed with zip, so in order to use them we should extract them, and because the zip file contains a data subdirectory, we rename that to lnb (after the domain name of the National Library of Latvia).\n1import urllib.request\nimport zipfile\nimport os\n\n2url = 'https://dati.lnb.lv/files/natl_bibliography-2014-2023-marc.zip'\ntarget_dir = 'raw-data'\ntarget_file = target_dir + '/lnb-natl_bibliography-2014-2023-marc.zip'\n\n3urllib.request.urlretrieve(url, target_file)\n\n4with zipfile.ZipFile(target_file, 'r') as zip_ref:\n    zip_ref.extractall(target_dir)\n\n5os.rename(target_dir + '/data', target_dir + '/lnb')\n\n1\n\nImport the Python libraries: urllib for the download, and zipfile for uncompressing\n\n2\n\ncreating variables\n\n3\n\ncall the download function with two arguments: the URL of the data source and the target file in our system\n\n4\n\nuncompress the zip file to our target directory\n\n5\n\nrename the data subdirectory (comes inside the zip file) to lnb\n\n\nAt the end of the process we will have a file raw-data/lnb/natl_bibliography-2014-2023-marc.xml, a set of MARC21 record in MARCXML serialization format, that is MARC21 in XML.\nThe code of this section is available at scripts/ch3a-download-a-file.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#apis-to-retrieve-records",
    "href": "ch3a.html#apis-to-retrieve-records",
    "title": "5  data acquisition",
    "section": "5.3 APIs to retrieve records",
    "text": "5.3 APIs to retrieve records\n\n5.3.1 OAI-PMH\n\nThe Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) is a low-barrier mechanism for repository interoperability. Data Providers are repositories that expose structured metadata via OAI-PMH. Service Providers then make OAI-PMH service requests to harvest that metadata. OAI-PMH is a set of six verbs or services that are invoked within HTTP. From https://www.openarchives.org/pmh/\n\nThe protocol provides 6 ‘verbs’ or endpoints to retreive information:\n\nGetRecord: to retrieve an individual metadata record\nIdentify: information about a repository\nListIdentifiers: to harvest record identifiers\nListMetadataFormats: to retrieve the metadata formats available from a repository. These formats are alternative XML representations of the same content, but it also happens that a library provides different content via different formats. In the ListIdentifiers and ListRecords verbs it is used as the metadataPrefix parameter. At least one format, the Dublin Core is mandatory.\nListRecords: to harvest records\nListSets: to retrieve the set structure of a repository. Sets are optional collections within a repository, e.g. there might be individual sets according to the physical collections of a library.\n\nThe response of these verbs is always XML. For the List* verbs the maintainer of the service sets a pager size, so you do not retrieve all records at once only limeted number (e.g. 100). The pagination is implemented by a distinct XML element called resumptionToken, which the client should use in the next call. Its value can change from call to call, so the client always have to extract is. In the above links you can find more detailed information and examples.\nThere are different implementations of a OAI-PMH clients in Python, we use Mathias Loesch’s Sickle module (version 0.7.0) to retrieve the Estonian National Bibliography from the DataLab, the data sharing platform of the Estonian National Library. At the time of writing it returns somewhat almost 410 000 MARCXML records. We save them into local files each containing 100 000 records. As OAI-PMH is based on XML technology we use lxml module (version 6.0.0), a lightweight XML and HTML processing toolbox.\nimport io\nimport sys\nimport os\nfrom lxml import etree\nfrom sickle import Sickle\n\n1if len(sys.argv) == 1:\n    print('Please give a directory name')\n    exit()\n2dir = sys.argv[1]\nif not os.path.exists(dir):\n    os.mkdir(dir)\nprint('saving to %s' % (dir))\n\n3namespaces = {\n    'oai': 'http://www.openarchives.org/OAI/2.0/',\n    'marc21': 'http://www.loc.gov/MARC21/slim'\n}\n\n4header = '&lt;?xml version=\"1.0\" encoding=\"utf8\"?&gt;' + \"\\n\" \\\n       + '&lt;collection&gt;' + \"\\n\"\nfooter = '&lt;/collection&gt;'\n\n5def write_output(xmlrecords, file_counter, dir_name):\n    \"\"\"\n    Writes MARC records to file\n    Parameters                              \n    ----------\n    xmlrecords : list\n        A list of string contains the individual MARCXML records\n    file_counter : int\n        A file name counter\n    dir_name : int\n        The name of the output directory\n    \"\"\"\n    file_name = \"%s/%06d.xml\" % (dir_name, counter)\n    print(file_name)\n    with io.open(file_name, 'w', encoding='utf8') as f:\n        f.write(header)\n        f.write(\"\\n\".join(xmlrecords) + \"\\n\")\n        f.write(footer)\n\n6sickle = Sickle('https://data.digar.ee/repox/OAIHandler', max_retries=4)\n\n7xmlrecords = []\nfile_counter = 0\nrecord_counter = 0\n\n8it = sickle.ListRecords(metadataPrefix='marc21xml', set='erb')\n9for content in it:\n10    tree = etree.ElementTree(content)\n\n    records = tree.xpath(\n11                  '/oai:record[*]/oai:metadata/marc21:record',\n                  namespaces=namespaces)\n12    for record in records:\n13        xmlrecord = etree\\\n14              .tostring(record, encoding='utf8', method='xml')\\\n15              .decode(\"utf-8\")\\\n16              .replace(\"&lt;?xml version='1.0' encoding='utf8'?&gt;\\n\", '')\n17        xmlrecords.append(xmlrecord)\n18        record_counter += 1\n\n19    if len(xmlrecords) &gt;= 100000:\n20        write_output(xmlrecords, file_counter, dir)\n21        xmlrecords = []\n        file_counter += 1\n\n22if len(xmlrecords) &gt; 0:\n    write_output(xmlrecords, file_counter, dir)\n23print('saved %d records to %d files' % (record_counter, file_counter))\n\n1\n\nWhen we run Python sys.argv list contains the arguments we passed to the interpreter. The len() function returns the number of elements in a collection, so it gives here the number of arguments. The first argument is the name of the currect script, so if the number here is zero it means that the script itself does not have any argument. We should pass at least one argument: the name of the directory where the script will store the records - if we don’t provide it, the script will send us a message and stops running (with the exit() function).\n\n2\n\nIf we provide an argument it will be stored as the name of the output directory. Then it creates the directory if it not already exists, and notify us.\n\n3\n\nnamespaces variable registers the XML namespaces the process needs - otherwise the XPath expresssions would not work.\n\n4\n\nheader and footer needs at the beginning and end of the output XML files. The XML files are hierarchical: their main content is a list of MARC records, and their parent element will be the &lt;collection&gt;.\n\n5\n\nwrite_output() function creates the output files in the specified directory. It prints the XML header, join together and prints the XML records, and finally prints the XML footer.\n\n6\n\nThe Sickle harvester is initialized with the endpoint of the OAI-PMH service and an extra argument to specify how many times it can retry to fetch a single URL. It is needed because sometimes there are communication problems between the server and the client (due to problems on either side or in the network).\n\n7\n\nInitialisation of the necessary variables: the collector of the individual records, the output file counter and the record counter.\n\n8\n\nStart the harvest using the ListRecords verb. It returns an iterator, so we should not take care of pagination with individual HTTP calls and resumptionToken mentioned above, it is handled automatically by the Sickle module. We use two arguments: we set the metadata schema as MARCXML and the set as erb which stands for the Estonian National Bibliography. These values can be extracted from a preliminary investigation of the supported values returned by the ListMetadataFormats and ListSets verbs.\n\n9\n\nIn the following the script iterates over the individual XML responses of the calls.\n\n10\n\nThe raw XML content is transormed to an XML element tree that is an internal data structure with an API.\n\n11\n\nSelection of the individual records with an XPath expression. For the expression we should provide the XML namespace. In OAI-PMH the individual records (&lt;oai:record&gt;) have two parts: a header, that contains record identifier and other metadata (such as the set it belongs to, the date of last modification) and the actual record (&lt;oai:metadata&gt;). This later is the container of the MARCXML record (&lt;marc21:record&gt;). If you would like to apply this script to another OAI-PMH server, please check the namespaces it uses. The abbreviations are not important, you can name them as you like (however to keep them as in the source, i.e. the XML response of the service helps in debugging the problems, if there are), but you should always use the URLs used by the service.\n\n12\n\nIterate over the indifidual records that xpath() returned.\n\n13\n\nTransformation of the above mentioned tree structure of the record to XML via a sequence of steps:\n\n14\n\ncreate the XML string…\n\n15\n\nconvert to UTF-8 encoding…\n\n16\n\nremove the XML declaration - we do not need it for each records.\n\n17\n\nadd the record into the record collection\n\n18\n\nincrease the record counter by one\n\n19\n\nif the record collection contains more than 100 000 records…\n\n20\n\nthe records should write to a file…\n\n21\n\nthen empty the collection, and increase file counter by one\n\n22\n\nafter the iteration if the record collection is not empty write it to a file\n\n23\n\nfinally notify the user about the number of records ingested and files written",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#download-an-sql-file",
    "href": "ch3a.html#download-an-sql-file",
    "title": "5  data acquisition",
    "section": "5.4 Download an SQL file",
    "text": "5.4 Download an SQL file\nSome bibliographic data sources are available as a relational database. These databases are distributed as so called SQL dump, a non binary text based file, that contains both the definition of the database structure (the properties of tables and columns), and the values. The dump files can be imported to the database. In this section we will import the following database:\n\nSimon Burrows and Mark Curran, The French Book Trade in Enlightenment Europe Database, 1769-1794 (https://fbtee.westernsydney.edu.au/stn/interface/, 6 May 2014)\n\nIt maps the trade of the Société Typographique de Neuchâtel, 1769-1794, and based on the almost intactly survided archive of the alliance of the printers at Neuchâtel (Switzerland). The database (henceforth FBTEE) could be imported into MySQL or its Open Source form MariaDB. the database is downloadable from https://fbtee.westernsydney.edu.au/main/download-database/.\nAs now we would like to download another file we might choose to copy the previous script, and replace the variables url and target_file with another one, but it would duplicate the usiness logic of the script. In software development there is a principle Don’t Repeat Yourself abbreviated as DRY. It suggest that we should avoid from code repetition, or other words: the code should be reusable, Instead of writing two (or more) specialized scripts that repeat the actual download and uncompressin part, we will create a function, that will be used by these scripts.\nFirst we will create a utility script that will contain functions we can use in the course. It will be called bdsutils (short for Bibliographic Data Science utilities). This is the\nimport urllib.request\nimport zipfile\n\n1target_dir = 'raw-data'\n\n2def download_and_unzip(url, target_file):\n3    \"\"\"\n    Download a zip file, and uncompress it\n    \n    Parameters\n    ----------\n    url : str\n        The URL of the zip file to download\n    sound : str\n        The local file name\n    \"\"\"\n\n    urllib.request.urlretrieve(url, target_file)\n\n    with zipfile.ZipFile(target_file, 'r') as zip_ref:\n        zip_ref.extractall(target_dir)\n\n1\n\nthe script keeps only one variable target_dir, that will remane the same accross calls\n\n2\n\nthe definition of the method\n\n3\n\nthe documentation of the method\n\n\n\n\n\n\n\n\nTODO: The code should be explained more!\n\n\n\nThe file is available in the repository as scripts/bdsutils.py.\nOnce we have this utility script, we can create the specific one for FBTEE. This zip file does not contain directories, only a single file chop_leeds_ac_uk.sql.\n1from bdsutils import target_dir, download_and_unzip\nimport os\n\n2url = 'https://fbtee.uws.edu.au/stn/database/download/STN_database.zip'\ntarget_file = target_dir + '/stn_database.zip'\n\n3download_and_unzip(url, target_file)\n\ndirectory = target_dir + '/fbtee'\n4if not os.path.exists(directory):\n    os.mkdir(directory)\n\n5os.rename(target_dir + '/chop_leeds_ac_uk.sql',\n          directory + '/chop_leeds_ac_uk.sql')\n\n1\n\nimport both the variable and the function from bdsutils (we should not add the .py extension). This way we should not use the prefix bdsutils for these components (such as bdsutils.target_dir)\n\n2\n\nthe definition of the FBTEE specific variables\n\n3\n\nusage of the method defined in bdsutils\n\n4\n\ncreate a dedicated directory for the database if it doesn’t already exist\n\n5\n\nmove the file to this directory\n\n\nIf you run it, you will have two files: - raw-data/fbtee/chop_leeds_ac_uk.sql - raw-data/stn_database.zip\nThe file is available in the repository as scripts/ch3a-download-stn.py.\n\n5.4.1 import BTEE to the database\nImporting a datadump into MySQL or MariaDB is a two step process. First you should create the database itself with permissions (for security reason it is advisable to create a dedicated user as well and not to use the default database user), then you can import the data.\nStep 1. Prepare the database\nLog in to MySQL, then create the database fbtee and a dedicated user (replace &lt;username&gt; and &lt;password&gt; with your choosen credentials):\nCREATE DATABASE fbtee;\n\nCREATE USER '&lt;username&gt;'@'localhost' IDENTIFIED BY '&lt;password&gt;';\nGRANT ALL PRIVILEGES ON fbtee.* TO '&lt;username&gt;'@'localhost' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\nquit\nNote: you can choose any name for the database, this material will use fbtee.\nStep 2. Import the data dump\nmysql -u &lt;username&gt; -p fbtee &lt; raw-data/fbtee/chop_leeds_ac_uk.sql\nThis command will ask for the password. Use the same credentials you set in Step 1. If you did not get any error message, you can check the database structure. Log in again with the dedicated credentials:\nmysql -u &lt;username&gt; -p fbtee\nThe you can check the list of tables with\nSHOW TABLES;\nthat returns\n+-------------------------------------+\n| Tables_in_fbtee                     |\n+-------------------------------------+\n| authors                             |\n| books                               |\n| books_authors                       |\n| books_call_numbers                  |\n| books_stn_catalogues                |\n| clients                             |\n| clients_addresses                   |\n| clients_correspondence_manuscripts  |\n| clients_correspondence_places       |\n| clients_people                      |\n| clients_professions                 |\n| keyword_assignments                 |\n| keyword_free_associations           |\n| keyword_tree_associations           |\n| keywords                            |\n| orders                              |\n| orders_agents                       |\n| orders_sent_via                     |\n| orders_sent_via_place               |\n| parisian_keywords                   |\n| parisian_system_keyword_assignments |\n| people                              |\n| people_professions                  |\n| places                              |\n| professions                         |\n| super_books                         |\n| super_books_keywords                |\n| tags                                |\n| transactions                        |\n| transactions_volumes_exchanged      |\n+-------------------------------------+\n30 rows in set (0,001 sec)\nor the structure of the books table with\nDESCRIBE books;\nThe result:\n+---------------------------+---------------+------+-----+---------+-------+\n| Field                     | Type          | Null | Key | Default | Extra |\n+---------------------------+---------------+------+-----+---------+-------+\n| book_code                 | char(9)       | NO   | PRI | NULL    |       |\n| super_book_code           | char(11)      | NO   |     | NULL    |       |\n| edition_status            | varchar(15)   | NO   |     | NULL    |       |\n| edition_type              | varchar(50)   | NO   |     | NULL    |       |\n| full_book_title           | varchar(750)  | YES  | MUL | NULL    |       |\n| short_book_titles         | varchar(1000) | YES  |     | NULL    |       |\n| translated_title          | varchar(750)  | YES  |     | NULL    |       |\n| translated_language       | varchar(50)   | YES  | MUL | NULL    |       |\n| languages                 | varchar(200)  | YES  | MUL | NULL    |       |\n| stated_publishers         | varchar(1000) | YES  | MUL | NULL    |       |\n| actual_publishers         | varchar(1000) | YES  |     | NULL    |       |\n| stated_publication_places | varchar(1000) | YES  |     | NULL    |       |\n| actual_publication_places | varchar(1000) | YES  |     | NULL    |       |\n| stated_publication_years  | varchar(1000) | YES  |     | NULL    |       |\n| actual_publication_years  | varchar(10)   | YES  |     | NULL    |       |\n| pages                     | varchar(250)  | YES  |     | NULL    |       |\n| quick_pages               | varchar(10)   | YES  |     | NULL    |       |\n| number_of_volumes         | int(11)       | YES  |     | NULL    |       |\n| section                   | varchar(10)   | YES  |     | NULL    |       |\n| edition                   | varchar(100)  | YES  |     | NULL    |       |\n| book_sheets               | varchar(200)  | YES  |     | NULL    |       |\n| notes                     | varchar(4000) | YES  |     | NULL    |       |\n| research_notes            | varchar(1000) | YES  |     | NULL    |       |\n+---------------------------+---------------+------+-----+---------+-------+\n23 rows in set (0,001 sec)\nThis material is not intend to provide introduction to SQL, if you are interested, we can suggest the Library Carpentry’s SQL lesson.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3b.html",
    "href": "ch3b.html",
    "title": "6  data validation",
    "section": "",
    "text": "abstract: Technical validity (XML, JSON validity check) and quality assessment.\nThe validity and quality of data are usually checked in two ways. Although there are schemas describing the structure of data and software that can be used to check whether a document complies with these schemas, in most cases these schemas are limited to describing only a general structure (for example, a MARC record contains control and data fields, the latter may contain indicators and subfields), i.e., they only affect the outermost layer of the data. It is therefore worth performing further checks – either using software available for the given format or using the Exploratory Data Analysis methodology. The simplest method is the so-called completeness check, which examines what data elements are found in the database and in what proportions.1 It is also worth examining the content of the most important data elements covered by the analyses to see how consistent they are in terms of form and content: how many different forms does the same person or geographical name appear in, or how were the dates recorded? By browsing through a frequency list of values occurring in a given data element, we can gain an understanding of the nature of the data and the harmonization tasks to be performed in the subsequent processing steps. Such a list can be coded; for example, Harald Klinke grouped the dates in the Museum of Modern Art database according to format patterns (four numbers, four numbers-four numbers, four numbers-two numbers, etc.), thus obtaining a more manageable sample list instead of many individual dates.2",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3b.html#footnotes",
    "href": "ch3b.html#footnotes",
    "title": "6  data validation",
    "section": "",
    "text": "see (Kruusmaa, Tinits, and Nemvalts 2025)↩︎\nhttps://x.com/HxxxKxxx/status/1066805548866289664↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3c.html",
    "href": "ch3c.html",
    "title": "7  preprocessing",
    "section": "",
    "text": "7.1 Creating data frame from Parquet file\nabstract: File formats, data structures, conversion, and data loss control.\nDuring preprocessing, we convert the imported files into a data structure that is more suitable for processing with standard data analysis methods (in Python, the most common is Pandas, and in R, it is the Tibble “data frame”). It may happen that we do not transform all data, but only certain records (for example, only 17th-century books from a national bibliography) or certain data elements (for example, we omit library identifiers and other administrative data elements).\nApache Parquet files can be read with the Pandas, but it needs an extra module that understands the format. We will use PyArrow module (version 23.0.0) that provides a Python API of Apache Arrow. You have to install it by\nReading a parquet file is very similar to reading CSV:\nThe result is a normal data frame, all functionalities work, such as counting the number of columns and rows:\nthe list of columns:\ndisplaying the first rows:",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>preprocessing</span>"
    ]
  },
  {
    "objectID": "ch3c.html#creating-data-frame-from-parquet-file",
    "href": "ch3c.html#creating-data-frame-from-parquet-file",
    "title": "7  preprocessing",
    "section": "",
    "text": "pip install pyarrow\n\nimport pandas as pd\n\nparquet_file = 'raw-data/lnb/natl_bibliography-2014-2023-marc.parquet'\ndf = pd.read_parquet(parquet_file, engine='auto')\n\nprint(df.shape)\n\nprint(df.columns)\n\nprint(df.head())",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>preprocessing</span>"
    ]
  },
  {
    "objectID": "ch3d.html",
    "href": "ch3d.html",
    "title": "8  data harmonisation",
    "section": "",
    "text": "abstract: normalization and data enrichment. The reproducible conversion into a data set suitable for quantitative humanities analysis.\nData maintained by others rarely fit in every respect to the specific analytical purpose for which we are preparing them. The data that are important to us must be harmonized, i.e., normalized (standardize, resolve contradictions, convert certain data types—such as particular text variables to numeric ones) and enriched (calculate derived data, such as page numbers, import data from external data sources). Below, we examine four such harmonization steps: the harmonization of dates, place names, persons, and concepts. The dates show a high degree of variation not only in the MoMe collection, but in almost every library catalog we find dates that differ from the format that is easy for programs to handle. For example, dates given in Roman numerals (“MDCCLXXX. [1780]”), in text form (“druk janvier 2016.”), according to the reign of a monarch (“Meiji 40”) or according to another calendar. Another problem is the handling of uncertain dates (e.g., “18–” and “18uu” in library catalogs both mean that the publication is from the 19th century). Due to the variety, the conversion is not trivial, but neither is deciding what to convert the dates to in the end. There are different approaches in certain areas (see, for example, archival standards or the practice of Europeana). As the latest proposal, the undate Python library1 created by the DHTech community stores the following data elements: the unchanged form of the date in the source, the calendar, the accuracy of the date, the earliest and latest normalized dates, and the duration – i.e., for the sake of consistency in retrieval, the date is always a time range. For place names, gazeeters are available for identification and, if necessary, for extra data elements required for map representation or the display of language variants. Among the most important ones are CERL Thesaurus,2 Getty Thesaurus of Geographic Names,3 and Geonames,4 which can be queried via APIs. Although these are rich databases built from many sources and thoroughly checked, practice shows that in almost every bibliographic source we will find name forms that are not recognized by these services, so these can be incorporated into our own database with some non-automated manual data refinement. The same procedure can be followed for individuals, but naturally using different services: VIAF (Virtual International Authority File),5 the CERL Thesaurus personal name database, ISNI (International Standard Name Identifier),6 Wikidata.7 It is important to note that any given database will naturally contain many more personal names than geographical names, so the hit rate is likely to be lower. The world of concepts is much more diverse than that of geographical and personal names. Although there are universal conceptual dictionaries (knowledge organization systems), there is virtually no library catalog whose records contain only the concepts of a single dictionary. Instead of specific dictionaries, we recommend using the BARTOC service (Basic Register of Thesauri, Ontologies and Classification)8 to find the dictionary that best suits your research questions. When discussing harmonization, it is essential to mention the categories of inaccurate, incomplete, subjective, and uncertain data.9 We have seen an example of inaccurate data and its handling in the case of dates. Incomplete data is when we do not know all the details, for example, not all authors of a work are listed, or there are gaps in the provenance history of an object. We can deduce some data, but it is very difficult to describe what does not exist. Subjective data refers to provenance, i.e., who made the statement in question. Such statements are often hypothetical and may even be contradictory. Finally, uncertain data is when the truthfulness of a statement is doubtful. An important part of the theories cited above is that the past is constructed, and the interpretation of sources also depends on the interpreter’s prior knowledge. Consequently, historical information systems must necessarily allow for the coexistence of contradictory interpretations, and instead of binary (true-false) logic, uncertainties could be described using probability values.10 For example, “Alexandre Dumas” (if no other information is available) could refer to either the father or the son (both writers)—the former being more likely, the value of which can be recorded in the database and used, for example, when sorting search results.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3d.html#footnotes",
    "href": "ch3d.html#footnotes",
    "title": "8  data harmonisation",
    "section": "",
    "text": "(Koeser et al. 2025)↩︎\nhttps://data.cerl.org/thesaurus/↩︎\nhttps://www.getty.edu/research/tools/vocabularies/tgn/index.html↩︎\nhttps://geonames.org↩︎\nhttps://viaf.org/↩︎\nhttps://isni.org/↩︎\nhttps://wikidata.org↩︎\nhttps://bartoc.org↩︎\n(Mariani 2023)↩︎\nThaller, Manfred, On vagueness and uncertainty in historical data = Ivory Tower blog, 2020. https://ivorytower.hypotheses.org/88.↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3e.html",
    "href": "ch3e.html",
    "title": "9  data analysis",
    "section": "",
    "text": "abstract: data analysis and data visualization with programming (Python, R) and specialized tools.\nThe most spectacular phase of the work process is data analysis.1 Here, the researcher translates his or her own research questions into the operations offered by the tools used. Two types of data are used in the analyses: on the one hand, the original or enriched data available after data harmonization (title, place of publication, language, number of pages), and on the other hand, calculated data based on their analysis (including font type and size, amount of paper used to produce the form, number of words, number of printed words per capita). Data analysis methods are not unique to bibliographic data science; they are general methods for which general and (to a lesser extent) specialized data science teaching materials in the humanities2 are available, as well as textbooks on quantitative history.3 The curriculum cannot, of course, cover every conceivable data science technique, but it should use examples to introduce the most common procedures used in the history of the discipline (the basics of statistics, time series analysis, data visualization including map representation, text analysis, network analysis).",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3e.html#footnotes",
    "href": "ch3e.html#footnotes",
    "title": "9  data analysis",
    "section": "",
    "text": "see, among other things, the literature cited in connection with the research questions.↩︎\n(Karsdorp, Kestemont, and Riddell 2021); (Klinke 2025); (Arnold and Tilton 2024); (Bátorfy 2024); and the Programming Historian site: https://programminghistorian.org.↩︎\n(Lemercier and Zalc 2019); (Feinstein and Thomas 2008); (Hudson and Ishizu 2017).↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3f.html",
    "href": "ch3f.html",
    "title": "10  dissemination",
    "section": "",
    "text": "abstract: dissemination of results. Publication of software and research data for reuse.\nThe final step in the work process is the dissemination of results, which includes traditional publication methods (papers, books, conference presentations) as well as newer approaches, such as the publication of software used in the process, the generated data, and data and software studies focusing specifically on these, blogging and microblogging, sharing presentation slides and recordings, and participating in professional organizations. I would like to emphasize the importance of special data sharing. Imagine the following scenario: a researcher has worked hard to enrich a popular data source with high research potential that is maintained by a public collection. Later, another researcher would like to use the same database for their research. If she is not familiar with the previous researcher’s work, she can start the data enrichment process from scratch. But even if the first researcher published his data enrichment, it is much more likely that the subsequent researchers will find and use the original database. To prevent this, researchers would need to return the modified data to the original data provider. Fortunately, MARC21, introduced in the 34th update in 20221 a data provenance subfield to distinguish between data recorded by the library and data recorded by the researcher (and it is available in most fields), which could be a theoretical remedy for the library’s legitimate demand to take responsibility for its own data. In the life sciences, researchers can use nanopublications to share data enrichment steps with libraries, which can then incorporate them into their catalogs without compromising their own responsibility and credibility. The second researcher can then work on the data-enriched version. In order to realize this vision, communication between the parties must be standardized, and the research community can play a coordinating role in this process.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch3f.html#footnotes",
    "href": "ch3f.html#footnotes",
    "title": "10  dissemination",
    "section": "",
    "text": "https://www.loc.gov/marc/up34bibliographic/bdapndxg.html↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "11  After the research",
    "section": "",
    "text": "The broader context. Professional communities, conferences, journals, continuing education opportunities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>After the research</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arnold, Taylor, and Lauren Tilton. 2024. Humanities\nData in R: Exploring\nNetworks, Geospatial Data,\nImages, and Text. 2nd ed. 2024.\nQuantitative Methods in the Humanities and\nSocial Sciences. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-62566-4.\n\n\nBátorfy, Attila. 2024. Adatvizualizáció. Elmélet, rendszer, módszer.\nBevezetés az adatok grafikus ábrázolásának elméletébe és gyakorlatába. Budapest: ELTE Eötvös Kiadó.\n\n\nBourdieu, Pierre. 2008. “A Conservative Revolution in\nPublishing.” Translation Studies 1 (2): 123–53. https://doi.org/10.1080/14781700802113465.\n\n\nFarkas, Farkas Gábor, János Káldos, and Péter Király. 2025. “A\nRégi Magyarországi Kiadványok „Sötét Anyaga”.” Magyar\nKönyvszemle 141 (2): 226–66. https://doi.org/10.17167/mksz.2025.2.226-266.\n\n\nFeinstein, Charles H., and Mark Thomas. 2008. Making History Count:\nA Primer in Quantitative Methods for Historians. Transferred to\ndigital printing (with corrections). Cambridge New York Melbourne Madrid\nCape Town Singapore São Paulo: Cambridge University Press.\n\n\nHeilbron, Johan. 1999. “Towards a Sociology of\nTranslation: Book Translations as\na Cultural World-System.”\nEuropean Journal of Social Theory 2 (4): 429–44. https://doi.org/10.1177/136843199002004002.\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th\nCentury? A Co-Occurrence Analysis of Early Modern\nGerman Dissertation Titles.” Synthese 203\n(2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations\nin French Libraries: The\nEMDFL Dataset.” Journal of Open\nHumanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nHudson, Pat, and Mina Ishizu. 2017. History by Numbers: An\nIntroduction to Quantitative Approaches. Second edition. London\nOxford New York New Delhi Sydney: Bloomsbury Academic.\n\n\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. 2021.\nHumanities Data Analysis: Case Studies with\nPython. Princeton Oxford: Princeton University Press.\n\n\nKirály, Péter. 2019. “Measuring Metadata\nQuality.” Doctoral dissertation, Göttingen:\nUniversity of Göttingen. https://doi.org/10.13140/RG.2.2.33177.77920.\n\n\nKirály, Péter, and András Kiséry. 2025. “‘Mór\nJókai, Alas’: The Most Successful\nHungarian Writer. A Quantitative\nAnalysis.” Patterns of Translation. https://translationpatterns.substack.com/p/mor-jokai-alas-the-most-successful.\n\n\nKirály, Péter, Tomasz Umerle, Vojtěch Malínek, Elżbieta Herden, Beata\nKoper, Giovanni Colavizza, Rindert Jagersma, et al. 2025. “Effects\nof Open Science and the Digital\nTransformation on the Bibliographical\nData Landscape.” In Library\nCatalogues as Data, edited by Paul\nGooding, Melissa Terras, and Sarah Ames, 1st ed., 19–44. Facet. https://doi.org/10.29085/9781783306602.004.\n\n\nKlinke, Harald. 2025. Cultural Data Science: An Introduction to\nR. Quantitative Methods in the Humanities and Social\nSciences. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-88130-5.\n\n\nKoeser, Rebecca Sutton, Julia Damerow, Robert Casties, and Cole\nCrawford. 2025. “Undate: Humanistic Dates for Computation:\nBecause Reality Is Frequently Inaccurate.”\nComputational Humanities Research 1: e5. https://doi.org/10.1017/chr.2025.10006.\n\n\nKruusmaa, Krister, Peeter Tinits, and Laura Nemvalts. 2025.\n“Curated Bibliographic Data:\nThe Case of the Estonian\nNational Bibliography.” Journal of\nOpen Humanities Data 11 (February): 16. https://doi.org/10.5334/johd.280.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019.\n“Bibliographic Data Science and the\nHistory of the Book (c. 1500–1800).”\nCataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.\n\n\nLemercier, Claire, and Claire Zalc. 2019. Quantitative Methods in\nthe Humanities: An Introduction. Translated by Arthur Goldhammer.\nCharlottesville: University of Virginia Press.\n\n\nMariani, Fabio. 2023. “Introducing VISU:\nVagueness, Incompleteness,\nSubjectivity, and Uncertainty in\nArt Provenance Data.” In\nProceedings of the Workshop on\nComputational Methods in the\nHumanities 2022, Vol-3602:63–84. Workshop\nProceedings. https://ceur-ws.org/Vol-3602/paper5.pdf.\n\n\nSzemes, Botond, and Kata Dobás. 2025. “A Visegrádi Országok\nDigitális Irodalmi Emlékezete : Wikipedia,\nWikidata – a Regionális Irodalomtörténet Új\nAlakzatai.” Irodalomtörténeti Közlemények 129 (2):\n191–212. https://doi.org/10.56232/itk.2025.2.04.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ap1.html",
    "href": "ap1.html",
    "title": "Appendix A — Books",
    "section": "",
    "text": "Just as there is no teaching material specifically focused on digital book history, there are no books on this subject either. Once again, we can only recommend books on related topics.\n\nArnold, Taylor, and Lauren Tilton. 2015. Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text. Springer International Publishing. ISBN 978-3-319-20702-5 (Quantitative Methods in the Humanities and Social Sciences)\nBilbro, Rebecca, Tony Ojeda, and Benjamin Bengfort. 2018. Applied Text Analysis with Python. 1st edition. O’Reilly Media, Inc. ISBN 978-1-4919-6303-6\nGavin, Michael. 2023. Literary Mathematics: Quantitative Theory for Textual Studies. Stanford University Press. ISBN 978-1-5036-3282-0 (Stanford Text Technologies.)\nGonzales, Brighid M. 2020. Systems Librarianship: A Practical Guide for Librarians. Bloomsbury Publishing. ISBN 979-8-8818-7951-8 (Practical Guides for Librarians)\nGooding, Paul, Melissa M. Terras, and Sarah Ames, eds. 2025. Library Catalogues as Data: Research, Practice and Usage. Facet Publishing. ISBN 978-1-78330-658-9\nKlinke, Harald. 2025. Cultural Data Science: An Introduction to R. Springer Nature Switzerland. ISBN 978-3-031-88130-5. (Quantitative Methods in the Humanities and Social Sciences)\nManovich, Lev. 2020. Cultural Analytics. The MIT Press. ISBN 978-0-262-03710-5\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. 2021. Humanities Data Analysis: Case Studies with Python. Princeton University Press. ISBN 978-0-691-17236-1\nCrymble, Adam. 2021. Technology and the Historian: Transformations in the Digital Age. University of Illinois Press. ISBN 978-0-252-08569-7 (Topics in the Digital Humanities)\nJockers, Matthew L., and Rosamond Thalken. 2020. Text Analysis with R: For Students of Literature. Springer International Publishing. ISBN 978-3-030-39643-5 (Quantitative Methods in the Humanities and Social Sciences)\nNelson, Catherine. 2024. Software Engineering for Data Scientists: From Notebooks to Scalable Systems. O’Reilly. ISBN 978-1-0981-3620-8",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Books</span>"
    ]
  },
  {
    "objectID": "ap2.html",
    "href": "ap2.html",
    "title": "Appendix B — Journals",
    "section": "",
    "text": "The following journals regularly publish studies that are relevant for bibliographical data science:\n\nCode4Lib Journal\nComputational Humanities Research\nCurrent Research in Digital History\nDigital Humanities Quarterly\nDigital Scholarship in the Humanities\nInternational Journal of Digital Humanities\nInternational Journal of Humanities and Arts Computing\nJournal of Cultural Analytics\nJournal of Computing and Cultural Heritage\nJournal of Digital History\nJournal of Open Humanities Data\nOpen Library of Humanities Journal\nTransformations: A DARIAH Journal\nZeitschrift für digitale Geisteswissenschaften",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Journals</span>"
    ]
  },
  {
    "objectID": "ap3.html",
    "href": "ap3.html",
    "title": "Appendix C — Conferences",
    "section": "",
    "text": "ADHO Digital Humanities\nDARIAH Annual Conference\nComputational Humanities Research\nDigital Humanities im deutschsprachigen Raum\nDigital Humanties Benelux\nSemantic Web in Libraries\nTheory and Practice in Digital Libraries\nJoint Conference on Digital Libraries",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Conferences</span>"
    ]
  },
  {
    "objectID": "ap4.html",
    "href": "ap4.html",
    "title": "Appendix D — Data sources",
    "section": "",
    "text": "D.1 United States of America",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  },
  {
    "objectID": "ap4.html#united-states-of-america",
    "href": "ap4.html#united-states-of-america",
    "title": "Appendix D — Data sources",
    "section": "",
    "text": "Library of Congress — https://www.loc.gov/cds/products/marcDist.php. MARC21 (UTF-8 and MARC8 encoding), MARCXML formats, open access. Alternative access point: https://www.loc.gov/collections/selected-datasets/?fa=contributor:library+of+congress.+cataloging+distribution+service.\nHarvard University Library — https://library.harvard.edu/open-metadata. MARC21 format, CC0. Institution specific features are documented here\nColumbia University Library — https://library.columbia.edu/bts/clio-data.html. 10M records, MARC21 and MARCXML format, CC0.\nUniversity of Michigan Library — https://www.lib.umich.edu/open-access-bibliographic-records. 1,3M records, MARC21 and MARCXML formats, CC0.\nUniversity of Pennsylvania Libraries — https://www.library.upenn.edu/collections/digital-projects/open-data-penn-libraries. Two datasets are available:\n\nCatalog records created by Penn Libraries 572K records, MARCXML format, CC0,\nCatalog records derived from other sources, 6.5M records, MARCXML format, Open Data Commons ODC-BY, use in accordance with the OCLC community norms.\n\nYale University — https://guides.library.yale.edu/c.php?g=923429. Three datasets are available:\n\nYale-originated records: 1.47M records, MARC21 format, CC0\nWorldCat-derived records: 6.16M records, MARC21 format, ODC-BY\nOther records (MARC21), independent of Yale and WorldCat, where sharing is permitted. 404K records, MARC21 format.\n\nNational Library of Medicine (NLM) catalogue records — https://www.nlm.nih.gov/databases/download/catalog.html. 4.2 million records, NLMXML, MARCXML and MARC21 formats. NLM Terms and Conditions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  },
  {
    "objectID": "ap4.html#germany",
    "href": "ap4.html#germany",
    "title": "Appendix D — Data sources",
    "section": "D.2 Germany",
    "text": "D.2 Germany\n\nDeutsche Nationalbibliothek — https://www.dnb.de/DE/Professionell/Metadatendienste/Datenbezug/Gesamtabzuege/gesamtabzuege_node.html (note: the records are provided in utf-8 decomposed). 23.9M records, MARC21 and MARCXML format, CC0.\nBibliotheksverbundes Bayern — https://www.bib-bvb.de/web/b3kat/open-data. 27M records, MARCXML format, CC0.\nLeibniz-Informationszentrum Technik und Naturwissenschaften Universitätsbibliothek (TIB) — https://www.tib.eu/de/forschung-entwicklung/entwicklung/open-data. (no download link, use OAI-PMH instead) Dublin Core, MARC21, MARCXML, CC0.\nK10plus-Verbunddatenbank (K10plus union catalogue of Bibliotheksservice-Zentrum Baden Würtemberg (BSZ) and Gemensamer Bibliotheksverbund (GBV)) — https://swblod.bsz-bw.de/od/. 87M records, MARCXML format, CC0.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  },
  {
    "objectID": "ap4.html#others",
    "href": "ap4.html#others",
    "title": "Appendix D — Data sources",
    "section": "D.3 Others",
    "text": "D.3 Others\n\nUniversiteitsbibliotheek Gent — https://lib.ugent.be/info/exports. Weekly data dump in Aleph Sequential format. It contains some Aleph fields above the standard MARC21 fields. ODC ODbL.\nToronto Public Library — https://opendata.tplcs.ca/. 2.5 million MARC21 records, Open Data Policy\nRépertoire International des Sources Musicales — https://opac.rism.info/index.php?id=8&id=8&L=1. 800K records, MARCXML, RDF/XML, CC-BY.\nETH-Bibliothek (Swiss Federal Institute of Technology in Zurich) — http://www.library.ethz.ch/ms/Open-Data-an-der-ETH-Bibliothek/Downloads. 2.5M records, MARCXML format.\nBritish library — http://www.bl.uk/bibliographic/datafree.html#m21z3950 (no download link, use z39.50 instead after asking for permission). MARC21, usage will be strictly for non-commercial purposes.\nTalis — https://archive.org/details/talis_openlibrary_contribution. 5.5 million MARC21 records contributed by Talis to Open Library under the ODC PDDL.\nOxford Medicine Online (the catalogue of medicine books published by Oxford University Press) — https://oxfordmedicine.com/page/67/. 1790 MARC21 records.\nFennica — the Finnish National Bibliography provided by the Finnish National Library — http://data.nationallibrary.fi/download/. 1 million records, MARCXML, CC0.\nBiblioteka Narodawa (Polish National Library) — https://data.bn.org.pl/databases. 6.5 million MARC21 records.\nMagyar Nemzeti Múzeum (Hungarian National Library) — https://mnm.hu/hu/kozponti-konyvtar/nyilt-bibliografiai-adatok, 67K records, MARC21, HUNMARC, BIBFRAME, CC0\nUniversity of Amsterdam Library — https://uba.uva.nl/en/support/open-data/data-sets-and-publication-channels/data-sets-and-publication-channels.html, 2.7 million records, MARCXML, PDDL/ODC-BY. Note: the record for books are not downloadable, only other document types. One should request them via the website.\nPortugal National Library — https://opendata.bnportugal.gov.pt/downloads.htm. 1.13 million UNIMARC records in MARCXML, RDF XML, JSON, TURTLE and CSV formats. CC0\nNational Library of Latvia National bibliography (2017–2020) — https://dati.lnb.lv/. 11K MARCXML records.\nOpen datasets of the Czech National Library — https://www.en.nkp.cz/about-us/professional-activities/open-data CC0\n\nCzech National Bibliography — https://aleph.nkp.cz/data/cnb.xml.gz\nNational Authority File — https://aleph.nkp.cz/data/aut.xml.gz\nOnline Catalogue of the National Library of the Czech Republic — https://aleph.nkp.cz/data/nkc.xml.gz\nUnion Catalogue of the Czech Republic — https://aleph.nkp.cz/data/skc.xml.gz\nArticles from Czech Newspapers, Periodicals and Proceedings — https://aleph.nkp.cz/data/anl.xml.gz\nOnline Catalogue of the Slavonic Library — https://aleph.nkp.cz/data/slk.xml.gz\n\nEstonian National Bibliography — as downloadable TSV or MARC21 via OAI-PMH https://digilab.rara.ee/en/datasets/estonian-national-bibliography/ CC0\nA bibliographic dataset of the Danish Royal Library (Det Kgl. Bibliotek) including Danish monographs from approx. 1600-1900. — https://loar.kb.dk/handle/1902/49107 849K records, MARCXML, PDM 1.0\n\nThanks, Johann Rolschewski, Phú, and Hugh Paterson III for their help in collecting this list! Do you know some more data sources? Please let me know.\nThere are two more datasource worth mention, however they do not provide MARC records, but derivatives:\n\nLinked Open British National Bibliography 3.2M book records in N-Triplets and RDF/XML format, CC0 license\nLinked data of Bibliothèque nationale de France. N3, NT and RDF/XML formats, Licence Ouverte/Open Licence",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  }
]