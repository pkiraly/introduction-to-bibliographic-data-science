[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bibliographic Data Science",
    "section": "",
    "text": "Preface\nThis book is an attempt to implement the ideas described in the essay An outline of an imagined training course on bibliographic data science published in Bibliographic data blog, 2026.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Preparation\nBibliographic data science is a relatively new interdisciplinary field of research that lies at the intersection of library science (or, more broadly, cultural heritage science), history and social sciences, and certain components of computer science. The objective of bibliographic data science is to establish previously hidden or possibly only suspected historical or collection trends based on data sources containing a (typically but not exclusively) large number of bibliographic records, ideally all those related to a given topic (e.g., national bibliographies), and on data science methods. Some of the field’s research questions:\nAlthough digital humanities education has developed dynamically in recent years, computer-based analysis of bibliographic sources is unfortunately rarely featured, and similarly absent from library science and IT education. In my opinion, this gap could be remedied by a new informal vocational training program that would appeal to those who are interested in some of the above issues and who already have some knowledge in one of the relevant fields (e.g., library science, cultural history, literary sociology, information technology). The analysis of records based on library bibliographic standards would probably also be of interest in library training. The training may take the form of a summer university or a seminar/course jointly organized by several university departments. Participants in the training could be university students or practicing professionals.\nIn this book we use the Python programming language. You should have a basic knowledge of the language and should know how to install it on your machine. In order to separate our environment from already installed Python modules, we use a virtual environment. To create it run the following:\nWhen you run the code in the book, you should first activate this virtual environment:\n… and when you finish the session, you should deactivate it:\nWhen we talk about installing a module you should do it within this environment, then you can use the standard Python module installation method:\nWe provide a list of modules used in this book, you can install them in a single step as:\nSome of the code examples run in the command line and written in bash, that is available by default in Linux and Mac machines. For Windows you can install it via WLS.\nWhen you enter this virtual Ubuntu the first time, you should give a user name (which might be the same or different as your Windows user name), and a password.\nYou can find more details and troubleshooting in the following documentation page: WSL Installation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#preparation",
    "href": "introduction.html#preparation",
    "title": "1  Introduction",
    "section": "",
    "text": "python -m venv venv\n\nsource venv/bin/activate\n\ndeactivate\n\nvenv/bin/pip install pandas\n\nvenv/bin/pip install -r requirements.txt\n\n\nOpen command line or PowerShell and enter:\n\nwsl --install -d Ubuntu\nwsl --set-default-version 2\nwsl --set-default ubuntu\n\nin Windows search enter Ubuntu and click on the Ubuntu icon, or in command line/PowerShell enter\n\nubuntu",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#a-note-on-code",
    "href": "introduction.html#a-note-on-code",
    "title": "1  Introduction",
    "section": "1.2 A note on code",
    "text": "1.2 A note on code\nThe code (Python, HTML, XML etc.) in this book is a bit formatted by adding spaces and line breaks in order to make it easier to understand. These changes neither affect the original intention of the code, nor the processing workflow. For the original format please check source code of the examples.\n\n\n\n\nBourdieu, Pierre. 2008. “A Conservative Revolution in Publishing.” Translation Studies 1 (2): 123–53. https://doi.org/10.1080/14781700802113465.\n\n\nFarkas, Farkas Gábor, János Káldos, and Péter Király. 2025. “A Régi Magyarországi Kiadványok „Sötét Anyaga”.” Magyar Könyvszemle 141 (2): 226–66. https://doi.org/10.17167/mksz.2025.2.226-266.\n\n\nHeilbron, Johan. 1999. “Towards a Sociology of Translation: Book Translations as a Cultural World-System.” European Journal of Social Theory 2 (4): 429–44. https://doi.org/10.1177/136843199002004002.\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th Century? A Co-Occurrence Analysis of Early Modern German Dissertation Titles.” Synthese 203 (2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations in French Libraries: The EMDFL Dataset.” Journal of Open Humanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nKirály, Péter. 2019. “Measuring Metadata Quality.” Doctoral dissertation, Göttingen: University of Göttingen. https://doi.org/10.13140/RG.2.2.33177.77920.\n\n\nKirály, Péter, and András Kiséry. 2025. “‘Mór Jókai, Alas’: The Most Successful Hungarian Writer. A Quantitative Analysis.” Patterns of Translation. https://translationpatterns.substack.com/p/mor-jokai-alas-the-most-successful.\n\n\nKirály, Péter, Tomasz Umerle, Vojtěch Malínek, Elżbieta Herden, Beata Koper, Giovanni Colavizza, Rindert Jagersma, et al. 2025. “Effects of Open Science and the Digital Transformation on the Bibliographical Data Landscape.” In Library Catalogues as Data, edited by Paul Gooding, Melissa Terras, and Sarah Ames, 1st ed., 19–44. Facet. https://doi.org/10.29085/9781783306602.004.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019. “Bibliographic Data Science and the History of the Book (c. 1500–1800).” Cataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.\n\n\nSzemes, Botond, and Kata Dobás. 2025. “A Visegrádi Országok Digitális Irodalmi Emlékezete : Wikipedia, Wikidata – a Regionális Irodalomtörténet Új Alakzatai.” Irodalomtörténeti Közlemények 129 (2): 191–212. https://doi.org/10.56232/itk.2025.2.04.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch2a.html",
    "href": "ch2a.html",
    "title": "3  Scientific theories and models",
    "section": "",
    "text": "Darnton and his followers, Bourdieu, quantitative history, computational and data models of historical events – Thibodeau and Thaller",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scientific theories and models</span>"
    ]
  },
  {
    "objectID": "ch2b.html",
    "href": "ch2b.html",
    "title": "4  Cultural heritage data models",
    "section": "",
    "text": "the work-expression-manifestation-item model and its branches, ontologies, archival data models",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cultural heritage data models</span>"
    ]
  },
  {
    "objectID": "ch3a.html",
    "href": "ch3a.html",
    "title": "5  data acquisition",
    "section": "",
    "text": "5.1 Creating directories\nabstract: Main types of bibliographic data and data sources (library catalogs, citation databases, research data repositories, historical sources). Methods of data acquisition (standards, application programming interfaces, and tools), information about data structure (metadata schemas and serialization formats), terms of use.\nDigital data can be retrieved in three main methods. The most convenient option is for these to be available as downloadable files (e.g., as reusable research data), however this type of data sharing is relatively rare. It is more common for data sources to be accessed through some kind of application programming interface. Various applications are available for the most common interfaces (OAI-PMH, Z39.50, SRW/SRU, SPARQL), so programming is not necessarily required, but time must be set aside to study the institution-specific settings and parameters of the interfaces. Finally, it is often the case that no previous opportunity was available. At this point, we extract the data from the HTML source of the website, assuming that the typographical formatting consistently indicates certain semantic elements17 – but in this case, it is worth consulting with the website operator to see if there are any other options not documented on the site. Whichever solution you choose, make sure that the data license allows reuse. After downloading the data, the first step is to import it. Programming libraries supporting various bibliographic formats are available; for MARC21, for example, there are ones for Java, Python, Go, JavaScript, R, PHP, and other programming languages. In this textbook we will work with Python, but the code could be adapted to other programming languages.\nThere are four main types of data sources for library history research:\nIn this textbook we will work with different directories:\nThe first programming task is to create these directories. To create them we use the os Python library that provides standard operating system functions, such as makedirs that creates a directory.\nThe code of this section is available at scripts/ch3a-create-directories.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#creating-directories",
    "href": "ch3a.html#creating-directories",
    "title": "5  data acquisition",
    "section": "",
    "text": "data: the data we created\nraw-data: our original data sources downloaded from data providers’ sites\nplots: the output of data visualization\n\n\n1import os\n\n2directories = ['data', 'raw-data', 'plots']\n\n3for directory in directories:\n4    if not os.path.exists(directory):\n5        os.makedirs(directory)\n\n1\n\nimports the os library\n\n2\n\ncreates a list with three strings, the names of the directories\n\n3\n\niterates the directories one by one. In each iteration the name of the current directory will be stored in the variable directory\n\n4\n\nchecks if the directory does not exist. If it exists the script skips the directory and takes the next one. If it doesn’t exist, it continue with the next command\n\n5\n\ncreates the directory (due to the test in the previous line only if it doesn’t exist already)",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#download-a-marcxml-file",
    "href": "ch3a.html#download-a-marcxml-file",
    "title": "5  data acquisition",
    "section": "5.2 Download a MARCXML file",
    "text": "5.2 Download a MARCXML file\nYou can find a list of downloadable library catalogues ḣere. Now download a relatively small file with MARC records of the Latvian National Bibliography (2014-2023) provided by the Open Data Portal of the National Library of Latvia. The files are compressed with zip, so in order to use them we should extract them, and because the zip file contains a data subdirectory, we rename that to lnb (after the domain name of the National Library of Latvia).\n1import urllib.request\nimport zipfile\nimport os\n\n2url = 'https://dati.lnb.lv/files/natl_bibliography-2014-2023-marc.zip'\ntarget_dir = 'raw-data'\ntarget_file = target_dir + '/lnb-natl_bibliography-2014-2023-marc.zip'\n\n3urllib.request.urlretrieve(url, target_file)\n\n4with zipfile.ZipFile(target_file, 'r') as zip_ref:\n    zip_ref.extractall(target_dir)\n\n5os.rename(target_dir + '/data', target_dir + '/lnb')\n\n1\n\nImports the Python libraries: urllib for the download, and zipfile for uncompressing\n\n2\n\ncreats variables\n\n3\n\ncalls the download function with two arguments: the URL of the data source and the target file in our system\n\n4\n\nuncompresses the zip file to our target directory\n\n5\n\nrenames the data subdirectory (comes inside the zip file) to lnb\n\n\nAt the end of the process we will have a file raw-data/lnb/natl_bibliography-2014-2023-marc.xml, a set of MARC21 record in MARCXML serialization format, that is MARC21 in XML.\nThe code of this section is available at scripts/ch3a-download-a-file.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#apis-to-retrieve-records",
    "href": "ch3a.html#apis-to-retrieve-records",
    "title": "5  data acquisition",
    "section": "5.3 APIs to retrieve records",
    "text": "5.3 APIs to retrieve records\n\n5.3.1 OAI-PMH\n\nThe Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) is a low-barrier mechanism for repository interoperability. Data Providers are repositories that expose structured metadata via OAI-PMH. Service Providers then make OAI-PMH service requests to harvest that metadata. OAI-PMH is a set of six verbs or services that are invoked within HTTP. From https://www.openarchives.org/pmh/\n\nThe protocol provides 6 ‘verbs’ or endpoints to retrieve information:\n\nGetRecord: to retrieve an individual metadata record\nIdentify: information about a repository\nListIdentifiers: to harvest record identifiers\nListMetadataFormats: to retrieve the metadata formats available from a repository. These formats are alternative XML representations of the same content, but it also happens that a library provides different content via different formats. In the ListIdentifiers and ListRecords verbs it is used as the metadataPrefix parameter. At least one format, the Dublin Core is mandatory.\nListRecords: to harvest records\nListSets: to retrieve the set structure of a repository. Sets are optional collections within a repository, e.g. there might be individual sets according to the physical collections of a library.\n\nThe response of these verbs is always XML. For the List* verbs the maintainer of the service sets a pager size, so you do not retrieve all records at once, only limited number (e.g. 100). The pagination is implemented by a distinct XML element called resumptionToken, which the client should use in the next call. Its value can change from call to call, so the client always have to extract is. In the above links you can find more detailed information and examples.\nThere are different implementations of OAI-PMH clients in Python, we use Mathias Loesch’s Sickle module (version 0.7.0) to retrieve the Estonian National Bibliography from the DataLab, the data sharing platform of the Estonian National Library. At the time of writing it returns almost 410 000 MARCXML records. We save them into local files each containing 100 000 records. As OAI-PMH is based on XML technology we use the lxml module (version 6.0.0), a lightweight XML and HTML processing toolbox.\nimport io\nimport sys\nimport os\nfrom lxml import etree\nfrom sickle import Sickle\n\n1if len(sys.argv) == 1:\n    print('Please give a directory name')\n    exit()\n2dir = sys.argv[1]\nif not os.path.exists(dir):\n    os.mkdir(dir)\nprint('saving to %s' % (dir))\n\n3namespaces = {\n    'oai': 'http://www.openarchives.org/OAI/2.0/',\n    'marc21': 'http://www.loc.gov/MARC21/slim'\n}\n\n4header = '&lt;?xml version=\"1.0\" encoding=\"utf8\"?&gt;' + \"\\n\" \\\n       + '&lt;collection&gt;' + \"\\n\"\nfooter = '&lt;/collection&gt;'\n\n1\n\nWhen we run Python, the sys.argv list contains the arguments we passed to the interpreter. The len() function returns the number of elements in a collection, so it gives here the number of arguments. The first argument is the name of the current script, so if the number here is zero it means that the script itself does not have any argument. We should pass at least one argument: the name of the directory where the script will store the records – if we don’t provide it, the script will send us a message and stop running (with the exit() function).\n\n2\n\nIf we provide an argument it will be stored as the name of the output directory. Then it creates the directory if it does not already exist, and notify us.\n\n3\n\nnamespaces variable registers the XML namespaces the process needs - otherwise the XPath expressions would not work.\n\n4\n\nheader and footer needs at the beginning and end of the output XML files. The XML files are hierarchical: their main content is a list of MARC records, and their parent element will be the &lt;collection&gt;.\n\n\nTo save the records we have to do several things, like naming an output file, insert header and footer. We should save records several times, so if we have such repeatable task, it is always suggested to create a method.\n1def write_output(xmlrecords, file_counter, dir_name):\n    \"\"\"\n    Writes MARC records to file\n    Parameters                              \n    ----------\n    xmlrecords : list\n        A list of string contains the individual MARCXML records\n    file_counter : int\n        A file name counter\n    dir_name : int\n        The name of the output directory\n    \"\"\"\n    file_name = \"%s/%06d.xml\" % (dir_name, counter)\n    print(file_name)\n    with io.open(file_name, 'w', encoding='utf8') as f:\n        f.write(header)\n        f.write(\"\\n\".join(xmlrecords) + \"\\n\")\n        f.write(footer)\n\n1\n\nA write_output() function creates the output files in the specified directory. It prints the XML header, join together and prints the XML records, and finally prints the XML footer.\n\n\nThen we create some variables to store the harvested records, and keeping status information, then initialize and start the harvester.\n1xmlrecords = []\nfile_counter = 0\nrecord_counter = 0\n\n2sickle = Sickle('https://data.digar.ee/repox/OAIHandler', max_retries=4)\n3it = sickle.ListRecords(metadataPrefix='marc21xml', set='erb')\n\n1\n\nInitialisation of the necessary variables: the collector of the individual records, the output file counter and the record counter.\n\n2\n\nThe Sickle harvester is initialized with the endpoint of the OAI-PMH service and an extra argument to specify how many times it can retry to fetch a single URL. It is needed because sometimes there are communication problems between the server and the client (due to problems on either side or in the network).\n\n3\n\nStart the harvest using the ListRecords verb. It returns an iterator, so we should not take care of pagination with individual HTTP calls and resumptionToken mentioned above, it is handled automatically by the Sickle module. We use two arguments: we set the metadata schema as MARCXML and the set as erb which stands for the Estonian National Bibliography. These values can be extracted from a preliminary investigation of the supported values returned by the ListMetadataFormats and ListSets verbs.\n\n\nIn the following the script iterates over the individual XML responses of the calls.\n1for content in it:\n2    tree = etree.ElementTree(content)\n\n    records = tree.xpath(\n3                  '/oai:record[*]/oai:metadata/marc21:record',\n                  namespaces=namespaces)\n4    for record in records:\n5        xmlrecord = etree\\\n6              .tostring(record, encoding='utf8', method='xml')\\\n7              .decode(\"utf-8\")\\\n8              .replace(\"&lt;?xml version='1.0' encoding='utf8'?&gt;\\n\", '')\n9        xmlrecords.append(xmlrecord)\n10        record_counter += 1\n\n11    if len(xmlrecords) &gt;= 100000:\n12        write_output(xmlrecords, file_counter, dir)\n13        xmlrecords = []\n        file_counter += 1\n\n1\n\nIterates over the responses. We do not need to call individual HTTP requests, the underlying library Sickle does it for us automatically.\n\n2\n\nThe raw XML content is transformed to an XML element tree that is an internal data structure with an API.\n\n3\n\nSelection of the individual records with an XPath expression. For the expression we should provide the XML namespace. In OAI-PMH the individual records (&lt;oai:record&gt;) have two parts: a header, that contains record identifier and other metadata (such as the set it belongs to, the date of last modification) and the actual record (&lt;oai:metadata&gt;). This later is the container of the MARCXML record (&lt;marc21:record&gt;). If you would like to apply this script to another OAI-PMH server, please check the namespaces it uses. The abbreviations are not important, you can name them as you like (however to keep them as in the source, i.e. the XML response of the service helps in debugging the problems, if there are), but you should always use the URLs used by the service.\n\n4\n\nIterate over the individual records that xpath() returned.\n\n5\n\nTransformation of the above mentioned tree structure of the record to XML via a sequence of steps:\n\n6\n\ncreate the XML string…\n\n7\n\nconvert to UTF-8 encoding…\n\n8\n\nremove the XML declaration – we do not need it for each record.\n\n9\n\nadd the record into the record collection\n\n10\n\nincrease the record counter by one\n\n11\n\nif the record collection contains more than 100 000 records…\n\n12\n\nthe records should be written to a file…\n\n13\n\nthen empty the collection, and increase file counter by one\n\n\nSince during the iteration we collect records, but do not save the list to file only when the list size reaches a limit, there is a chance that when the iteraton ends we still have unsaved record. So we should save them and print a report.\n1if len(xmlrecords) &gt; 0:\n    write_output(xmlrecords, file_counter, dir)\n2print('saved %d records to %d files' % (record_counter, file_counter))\n\n1\n\nafter the iteration if the record collection is not empty write it to a file\n\n2\n\nfinally notify the user about the number of records ingested and files written",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#download-an-sql-file",
    "href": "ch3a.html#download-an-sql-file",
    "title": "5  data acquisition",
    "section": "5.4 Download an SQL file",
    "text": "5.4 Download an SQL file\nSome bibliographic data sources are available as a relational database. These databases are distributed as a so-called SQL dump, a non binary text based file, that contains both the definition of the database structure (the properties of tables and columns), and the values. The dump files can be imported to the database. In this section we will import the following database:\n\nSimon Burrows and Mark Curran, The French Book Trade in Enlightenment Europe Database, 1769-1794 (https://fbtee.westernsydney.edu.au/stn/interface/, 6 May 2014)\n\nIt maps the trade of the Société Typographique de Neuchâtel, 1769-1794, and based on the almost intactly surviving archive of the alliance of the printers at Neuchâtel (Switzerland). The database (henceforth FBTEE) could be imported into MySQL or its Open Source form MariaDB. the database is downloadable from https://fbtee.westernsydney.edu.au/main/download-database/.\nAs now we would like to download another file we might choose to copy the previous script, and replace the variables url and target_file with another one, but it would duplicate the business logic of the script. In software development there is a principle Don’t Repeat Yourself abbreviated as DRY. It suggests that we should avoid code repetition, or in other words: the code should be reusable. Instead of writing two (or more) specialized scripts that repeat the actual download and uncompressing part, we will create a function that will be used by these scripts.\nFirst we will create a utility script that will contain functions we can use in the course. It will be called bdsutils (short for Bibliographic Data Science utilities). This is the\nimport urllib.request\nimport zipfile\n\n1target_dir = 'raw-data'\n\n2def download_and_unzip(url, target_file):\n3    \"\"\"\n    Download a zip file, and uncompress it\n    \n    Parameters\n    ----------\n    url : str\n        The URL of the zip file to download\n    sound : str\n        The local file name\n    \"\"\"\n\n    urllib.request.urlretrieve(url, target_file)\n\n    with zipfile.ZipFile(target_file, 'r') as zip_ref:\n        zip_ref.extractall(target_dir)\n\n1\n\nthe script keeps only one variable target_dir, that will remain the same across calls\n\n2\n\nthe definition of the method\n\n3\n\nthe documentation of the method\n\n\n\n\n\n\n\n\nTODO: The code should be explained more!\n\n\n\nThe file is available in the repository as scripts/bdsutils.py.\nOnce we have this utility script, we can create the specific one for FBTEE. This zip file does not contain directories, only a single file chop_leeds_ac_uk.sql.\n1from bdsutils import target_dir, download_and_unzip\nimport os\n\n2url = 'https://fbtee.uws.edu.au/stn/database/download/STN_database.zip'\ntarget_file = target_dir + '/stn_database.zip'\n\n3download_and_unzip(url, target_file)\n\ndirectory = target_dir + '/fbtee'\n4if not os.path.exists(directory):\n    os.mkdir(directory)\n\n5os.rename(target_dir + '/chop_leeds_ac_uk.sql',\n          directory + '/chop_leeds_ac_uk.sql')\n\n1\n\nimports both the variable and the function from bdsutils (we should not add the .py extension). This way we should not use the prefix bdsutils for these components (such as bdsutils.target_dir)\n\n2\n\nthe definition of the FBTEE specific variables\n\n3\n\nusage of the method defined in bdsutils\n\n4\n\ncreates a dedicated directory for the database if it doesn’t already exist\n\n5\n\nmoves the file to this directory\n\n\nIf you run it, you will have two files: - raw-data/fbtee/chop_leeds_ac_uk.sql - raw-data/stn_database.zip\nThe file is available in the repository as scripts/ch3a-download-stn.py.\n\n5.4.1 import BTEE to the database\nImporting a datadump into MySQL or MariaDB is a two step process. First you should create the database itself with permissions (for security reasons it is advisable to create a dedicated user as well and not to use the default database user), then you can import the data.\nStep 1. Prepare the database\nLog in to MySQL, then create the database fbtee and a dedicated user (replace &lt;username&gt; and &lt;password&gt; with your chosen credentials):\nCREATE DATABASE fbtee;\n\nCREATE USER '&lt;username&gt;'@'localhost' IDENTIFIED BY '&lt;password&gt;';\nGRANT ALL PRIVILEGES ON fbtee.* TO '&lt;username&gt;'@'localhost' WITH GRANT OPTION;\nFLUSH PRIVILEGES;\nquit\nNote: you can choose any name for the database, this material will use fbtee.\nStep 2. Import the data dump\nmysql -u &lt;username&gt; -p fbtee &lt; raw-data/fbtee/chop_leeds_ac_uk.sql\nThis command will ask for the password. Use the same credentials you set in Step 1. If you did not get any error message, you can check the database structure. Log in again with the dedicated credentials:\nmysql -u &lt;username&gt; -p fbtee\nThe you can check the list of tables with\nSHOW TABLES;\nthat returns\n+-------------------------------------+\n| Tables_in_fbtee                     |\n+-------------------------------------+\n| authors                             |\n| books                               |\n| books_authors                       |\n| books_call_numbers                  |\n| books_stn_catalogues                |\n| clients                             |\n| clients_addresses                   |\n| clients_correspondence_manuscripts  |\n| clients_correspondence_places       |\n| clients_people                      |\n| clients_professions                 |\n| keyword_assignments                 |\n| keyword_free_associations           |\n| keyword_tree_associations           |\n| keywords                            |\n| orders                              |\n| orders_agents                       |\n| orders_sent_via                     |\n| orders_sent_via_place               |\n| parisian_keywords                   |\n| parisian_system_keyword_assignments |\n| people                              |\n| people_professions                  |\n| places                              |\n| professions                         |\n| super_books                         |\n| super_books_keywords                |\n| tags                                |\n| transactions                        |\n| transactions_volumes_exchanged      |\n+-------------------------------------+\n30 rows in set (0,001 sec)\nor the structure of the books table with\nDESCRIBE books;\nThe result:\n+---------------------------+---------------+------+-----+---------+-------+\n| Field                     | Type          | Null | Key | Default | Extra |\n+---------------------------+---------------+------+-----+---------+-------+\n| book_code                 | char(9)       | NO   | PRI | NULL    |       |\n| super_book_code           | char(11)      | NO   |     | NULL    |       |\n| edition_status            | varchar(15)   | NO   |     | NULL    |       |\n| edition_type              | varchar(50)   | NO   |     | NULL    |       |\n| full_book_title           | varchar(750)  | YES  | MUL | NULL    |       |\n| short_book_titles         | varchar(1000) | YES  |     | NULL    |       |\n| translated_title          | varchar(750)  | YES  |     | NULL    |       |\n| translated_language       | varchar(50)   | YES  | MUL | NULL    |       |\n| languages                 | varchar(200)  | YES  | MUL | NULL    |       |\n| stated_publishers         | varchar(1000) | YES  | MUL | NULL    |       |\n| actual_publishers         | varchar(1000) | YES  |     | NULL    |       |\n| stated_publication_places | varchar(1000) | YES  |     | NULL    |       |\n| actual_publication_places | varchar(1000) | YES  |     | NULL    |       |\n| stated_publication_years  | varchar(1000) | YES  |     | NULL    |       |\n| actual_publication_years  | varchar(10)   | YES  |     | NULL    |       |\n| pages                     | varchar(250)  | YES  |     | NULL    |       |\n| quick_pages               | varchar(10)   | YES  |     | NULL    |       |\n| number_of_volumes         | int(11)       | YES  |     | NULL    |       |\n| section                   | varchar(10)   | YES  |     | NULL    |       |\n| edition                   | varchar(100)  | YES  |     | NULL    |       |\n| book_sheets               | varchar(200)  | YES  |     | NULL    |       |\n| notes                     | varchar(4000) | YES  |     | NULL    |       |\n| research_notes            | varchar(1000) | YES  |     | NULL    |       |\n+---------------------------+---------------+------+-----+---------+-------+\n23 rows in set (0,001 sec)\nThis material is not intended to provide an introduction to SQL, if you are interested, we can suggest the Library Carpentry’s SQL lesson.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#web-scraping",
    "href": "ch3a.html#web-scraping",
    "title": "5  data acquisition",
    "section": "5.5 Web scraping",
    "text": "5.5 Web scraping\nOftentimes it happens that an interesting bibliographic data source does not provide any download or programmable access option. We have two options here. The first is to contact the data provider and ask help to access the data. If we can not find contact information, or we do not receive any answer to our question, the second option is to extract data from the public web site, called screen scraping or web scraping. In this section the method will be demonstrated on UNESCO’s Index Translationum database (IT).\nIT collected bibliographical records describing translated books in any genre. These records came from national libraries between 1978 and roughly 2009 (IT never announced the cease of data collection, but practically records after this year are quite rare). Before 1978 IT had been published in printed volumes, however I am not aware of digitised versions. Another important problem is that the IT website is not always available.\nThe database does not provide browsing functionality, so one should start with a query, such as looking for translations of an author, or translations from a particular language (specifying the source language). The search queries (expressed as URLs) look like these:\n\nfor authors: https://www.unesco.org/xtrans/bsresult.aspx?a=[author], where [author] should contain the URL encoded name of the author in family name, first name format (e.g. William Shakespeare should be encoded as Shakespeare,%20William)\nfor source language: https://www.unesco.org/xtrans/bsresult.aspx?sl=[language code]&tie=a, where the [language code] should be substituted with a concrete language code, such as hun for Hungarian\n\nYou can play at the user interface to find the good initial query term, then you can start with the code (note: the URL encoding is done by the web form, but if you do it programmatically you have to take care with your programming language).\nA note on terminology: in translation study the language from which the work has been written is called the source language, and the translation is written in the target language. Sometimes the translator does not work from the source language, but from another translation, the language of which is called the intermediate language.\nA note on code: every HTML code in this section is a bit formatted by adding spaces and line breaks in order to make it easier to understand. For the original format please check IT’s HTML source.\nThe result is a HTML page that you have to parse and extract data. The first thing we extract now is pagination. You receive only 10 results for a single query. The next set can be accessed by clicking on the right arrow icon representing the link to the next page. Fortunately IT applies the class attribute of the HTML element to inject some semantics into the page, and we will utilize this feature. Here is the next link’s HTML structure:\n&lt;td class=”next”&gt;\n  &lt;a href=”bsresult.aspx?lg=0&amp;a=Shakespeare, William&amp;fr=10”&gt;\n    &lt;img border=”0” src=”Images/cy_r_arr.gif” alt=”prev”&gt;\n  &lt;/a&gt;\n&lt;/td&gt;\nThe class=”next” part is unique in the page, however on the last page it is empty, since there are no more hits. Thus we should parse it in two steps: first get the content of the &lt;td&gt; element, then we should find inside the &lt;a&gt; element’s href attribute. Since it contains a relative link, in order to fetch it, we should add https://www.unesco.org/xtrans/ to the beginning. We can continue the iteration until the td element becomes empty.\nThe next thing is to parse the content, the individual bibliographical descriptions. The results are displayed in a table structure, where the table has the following structure:\n&lt;table class=”restable”&gt;\n  &lt;tr&gt;\n    &lt;td class=”res1”&gt;1/4380&lt;/td&gt;\n    &lt;td class=”res2”&gt;[bibliographical description]&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td class=”res1”&gt;2/4380&lt;/td&gt;\n    &lt;td class=”res2”&gt;[bibliographical description]&lt;/td&gt;\n  &lt;/tr&gt;\n  ...\n&lt;/table&gt;\nso we should find the table with class=”restable”, then inside it we should process the bibliographical descriptions, which always take place in a cell (&lt;td&gt; element) with class=”res2”.\nThanks to the unknown software designers and developers behind IT, all elements of the bibliographical description are in a semantically identifiable &lt;span&gt; element. Here is an example:\n&lt;td class=”res2”&gt;\n  &lt;span class=”sn_auth_name”&gt;Shakespeare&lt;/span&gt;,\n  &lt;span class=”sn_auth_firstname”&gt;William&lt;/span&gt;:\n  &lt;span class=”sn_target_title”&gt;König Lear: Tragödie&lt;/span&gt;\n  [&lt;span class=”sn_target_lang”&gt;German&lt;/span&gt;]\n  / &lt;span class=”sn_transl_name”&gt;Baudissin&lt;/span&gt;,\n  &lt;span class=”sn_transl_firstname”&gt;Wolf Heinrich&lt;/span&gt;\n  / &lt;span class=”sn_pub”&gt;&lt;span class=”place”&gt;Stuttgart&lt;/span&gt;:\n  &lt;span class=”publisher”&gt;Reclam&lt;/span&gt;\n  [&lt;span class=”sn_country”&gt;Germany&lt;/span&gt;]&lt;/span&gt;,\n  &lt;span class=”sn_year”&gt;1979&lt;/span&gt;.\n  &lt;span class=”sn_pagination”&gt;112 p.&lt;/span&gt;\n  &lt;span class=”sn_orig_title”&gt;King Lear&lt;/span&gt;\n  [&lt;span class=”sn_orig_lang”&gt;English&lt;/span&gt;]\n&lt;/td&gt;\nThe IT records come with the following vocabulary:\n\nplace: publication place\npublisher: publisher\nsn_auth_name: author’s family name\nsn_auth_firstname: author’s first name\nsn_target_title: the title in the target language\nsn_target_lang: the target language\nsn_country: country of the publication place\nsn_year: publication year\nsn_pagination: pagination\nsn_orig_lang: original (or source) langue\nsn_transl_name: translator’s family name\nsn_transl_firstname: translator’s first name\nsn_orig_title: title in source languge\nsn_editionstat: edition statement\n_sn_interm_titl_e: title in intermediate language\nsn_interm_lang: intermediate language\nsn_isbn: ISBN\nsn_auth_quality: quality indicator for the author\nsn_transl_quality: quality indicator for the translator\n\nOf course not all data elements are available in every record. IT collected bibliographical records from national libraries, and the quality of them vary a lot. There are character encoding problems, and sometimes falsely tagged data elements.\nThere are a number of technical possibilities to parse HTML. Since IT’s HTML pages are seemingly transformed from a database with strict rules, one can apply regular expressions, which is a risky approach in lots of other cases. There are more advanced HTML parser libraries however such as BeautifulSoap. Here is the documentation, and Programming Historian also has a tutorial:\n\nJeri Wieringa, “Intro to Beautiful Soup,” Programming Historian 1 (2012), https://doi.org/10.46430/phen0008\n\nwhich unfortunately a ‘retired’ lesson, since the HTML page on which the author demonstrates the strength of the library is no longer available, but still a good introduction, and one can apply the methods on IT.\n1import requests\n2import lxml.html\nimport os\n3import re\n4import urllib.parse\n5import csv\n6from collections import Counter\n\n1\n\nWe use another URL handling library: the request library that provides more convenient methods for the same task as the urllib package. The request package needs installation with a pip install request.\n\n2\n\nPreviously we used another module from the lxml library. html provides methods for parsing HTML files.\n\n3\n\nre provides regular expression functionalities. As this part of core Python, it doesn’t need installation.\n\n4\n\nthe parse module helps to parse URLs and extract important parameters\n\n5\n\nThe csv module helps in reading and writing CSV files.\n\n6\n\nCounter, well it is counting things\n\n\nWe define two variables for our URL requests.\nbase_url = 'https://www.unesco.org/xtrans/bsresult.aspx'\nit_params = {\n    'a': 'Bourdieu, Pierre',\n    'fr': 0\n}\nbase_url is part of the URL we always use in the requests. To retrieve the second and any following pages of the hit list we should change the rest of the URL, the so-called query part (after the question mark). It is a list of key=value pairs separated by ampersand (&) character. We set up the it_params variable, which represents the URL query. This is a dict data type, and will be automatically transformed to URL during the request.\nWe should do some more preparation. When we parse HTML pages the process is almost always iterative, because we usually move forward with baby steps adding new and new parts to our script. In practice it means that we fetch the website many times – that might cause problems on the data provider’s web site. A best practice is to be nice to them, and try to eliminate the number of requests by caching: when we first retrieve a page, we should store it locally as a file in our disk, and next time we read it from the disk.\nAdd a new function to our bsutils module:\nimport os\n...\ndef create_dir(dir):\n    \"\"\"\n    Create a directory if it doesn't yet exist\n    \n    :param dir: the directory name\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\nThen we use it in the web scraping script:\n1import bsutils\n...\n2create_dir('cache')\n3cache_dir = os.path.join('cache', 'it')\ncreate_dir(cache_dir)\n\n1\n\nimports our module\n\n2\n\ncreates a directory called ‘cache’\n\n3\n\ndefines a directory called it inside the cache directory. Different operating systems handle subdirectories differently, os.path.join provides a safe way to use the appropriate syntax.\n\n\nWe also have to create containers for the data we plan to extract.\n1records = []\n2authors = []\ntranslators = []\n3record_counter = 1\n4field_names = Counter({})\n\n1\n\nrecord (a list of dictionaries) will contain our records.\n\n2\n\nas there might be multiple authors, and translators, we collect them separately. Records will have an id field (IT does not provide one), and these two entities will refer to that identifier as the external keys in relational databases.\n\n3\n\nrecord_counter is the source of this identifier. When we process a new record, we will increment this value\n\n4\n\nduring the development we do not always know each field beforehand. Counter is a special Python data type that provides us a method to count things.\n\n\nWe start the process with the initial values:\nrequest_page(it_params)\nthat triggers the following:\ndef request_page(it_params):\n1    cache_file = os.path.join(cache_dir, f'results_{it_params['fr']}.html')\n2    if os.path.exists(cache_file):\n3        with open(cache_file, 'r') as f:\n            content = f.read()\n        process_page(content)\n    else:\n4        response = requests.get(base_url, params=it_params)\n5        if response.status_code == 200:\n6            content = response.text\n7            if re.search('The requested URL was rejected.', content) is None:\n8                with open(cache_file, 'wb') as f:\n                    f.write(response.content)\n9            process_page(content)\n\n1\n\ndefines the cache file name out of the current URL’s fr parameter (the offset of the first record in the page from the first record in the hit list).\n\n2\n\nchecks if the cache file already exists\n\n3\n\nif it does, the content should be read from the file, then process it with process_page() method.\n\n4\n\notherwise we should request the page from the IT server\n\n5\n\nif it returns with HTTP status code 200 – which means the communication went well\n\n6\n\nextracts the content\n\n7\n\nhowever, the IT web server also returns code 200 even though there are some errors, for example if we issue too many requests (which is now prevented by the caching mechanism). During the development we met only one error message. re.search(pattern, string) returns mathing objects if pattern is found in the string, otherwise it returns None.\n\n8\n\nif it doesn’t find the error message, the content will be saved to the cache file.\n\n9\n\nprocesses it with process_page() method.\n\n\ndef process_page(content):\n1    doc = lxml.html.fromstring(content)\n2    extract_translations(doc)\n3    from_param = extract_next_link(doc)\n4    if from_param != None:\n5        it_params['fr'] = from_param\n6        request_page(it_params)\n\n1\n\nthe lxml library parses the document into an internal object\n\n2\n\nextract_translations() method will use it to extract the records from this object\n\n3\n\nextract_next_link() method extracts the above mentioned from (fr) parameter from the “next” link on the page.\n\n4\n\nif it is not None\n\n5\n\nupdate the parameter holder\n\n6\n\nand call request_page() to process the next page.\n\n\nSo these two functions call each other until there is a “next” link on the page.\nef extract_translations(doc):\n1    global record_counter, authors, translators\n2    items = doc.findall('body/table[@class=\"restable\"]/tr/td[@class=\"res2\"]', {})\n    for item in items:\n3        record = {'id': record_counter}\n4        spans = item.findall('span')\n        for span in spans:\n5            key = span.get('class')\n6            if key == 'sn_pub':\n                subs = span.findall('span')\n                for sub in subs:\n7                    extract_key_value(record, sub)\n            else:\n8                extract_key_value(record, span)\n9        record, record_authors, record_translators = normalize_record(record)\n10        records.append(record)\n11        authors += record_authors\n        translators += record_translators\n12        record_counter += 1\n\n1\n\nthe method uses some global variables, we make it possible with the global keyword\n\n2\n\nextracts individual table cells with an XPath expression (explained above). For each HTML snippet…\n\n3\n\ncreates a dictionary for the record structure, initially only with the identifier\n\n4\n\nall the important data elements are in &lt;span&gt; elements, so we have to find extract them (inside the current cell), and iterate over them\n\n5\n\nthe semantic information is found in the class attribute, so we should save it\n\n6\n\nhowever there is a special span: sn_pub is a container of other spans, and we have to run a second iteration to extract its children\n\n7\n\nand 8. at the end we call the extract_key_value() method with the record structure and the current (non complex) span element. It extracts the value and stores it in the record\n\n8\n\nditto\n\n9\n\nafter processing all spans we should normalize the record with the normalize_record() method. It returns the normalized method, the list of authors, authors and translators\n\n10\n\nas record is a single dictionary it can be appended easily to the list of records\n\n11\n\nhowever as authors and translators are list of dictionaries, append() can not be used here, we should concatenate the lists\n\n12\n\nincreases the record counter by one\n\n\ndef extract_key_value(record, span):\n1    key = re.sub(r'^sn_', '', span.get('class'))\n2    if key not in ['auth_name', 'auth_firstname', 'transl_name', 'transl_firstname']:\n        field_names.update([key])\n3    if key not in record:\n        record[key] = []\n4    record[key].append(span.text)\n\n1\n\nthe key of the field is derived from the class attribute, but the ‘sn_’ string from its beginning should be removed with a regular expression. re.sub has three mandatory parameters: pattern, replacement and the input string. ^ is a building block of regular expressions, meaning the beginning of the string, so it will not match same string elsewhere (see the full syntax here)\n\n2\n\nauthor and translator names will be stored in a distinct table, but we count how many times the rest appear in the pages.\n\n3\n\nwe expect that each field is repeatable, so if the key is not available in the dictionary, we initialize it as a list\n\n4\n\nand append the value (the text attribute of the span) to this list\n\n\ndef extract_next_link(doc):\n1    next = doc.findall('body/table[@class=\"nav\"]/tr[1]/td[@class=\"next\"]/a', {})\n2    if len(next) &gt; 0:\n3        link = next[0].get('href')\n4        parsed_link = urllib.parse.urlparse(link)\n5        parameters = urllib.parse.parse_qs(parsed_link.query)\n6        if 'fr' in parameters and len(parameters['fr']) == 1:\n7            return parameters['fr'][0]\n8    return None\n\n1\n\nextracts the “next” link\n\n2\n\nit is not available on the last page, so it should be checked if it is processable\n\n3\n\nthe fr parameter is available in the href attribute of the first (and only) “next” link\n\n4\n\nparses it with the urllib’s method, that returns a fixed structure: 6-item named tuple.\n\n5\n\nits element query contains only a string, which should be parsed with parse_qs\n\n6\n\nthe keys in the query string are repeatable, and parse returns a dictionary. Make sure that the parameters you look for is available and has only one value\n\n7\n\nreturns the first (and only) fr value\n\n8\n\nin every other cases returns a special value: None (not above we had a check on this None)\n\n\nAs mentioned above there are two things we should improve in the extracted record structure:\n\nwe would store the author and translators in distinct CSV files (and we will join tables only during data analysis)\nthe values of the other fields are also lists, however we do not necessary need them as list, and in CSV it is not easy to handle lists in cells\n\ndef normalize_record(record):\n1    for key, value in record.items():\n2        if key not in ['auth_name', 'auth_firstname', 'transl_name', 'transl_firstname']:\n3            record[key] = ', '.join(value)\n\n4    authors = extract_names(record, 'auth_name', 'auth_firstname')\n    translators = extract_names(record, 'transl_name', 'transl_firstname')\n\n5    return record, authors, translators\n\n1\n\niterates over the key-value pairs of the dictionary\n\n2\n\nif the key is not one of the author and translator name related field\n\n3\n\n“flatten” it by concatenate the value with comma (and space)\n\n4\n\nhandles author and translator names with the extract_names() method, that returns the list of authors, and the list of translators\n\n5\n\nreturns the cleared record, and the two name lists\n\n\nThe extract_names() method however is not that simple as it seems to be. There are two problems.\n\nin HTML we have sequences of last name, first name pairs, while our structure collects all last names and first names in distinct lists\nsometimes the first name pair is missing. It would not be the problem if it would be the last element, but there are cases when an ‘et al.’ (as last name) takes place in the middle of the list. Because of this problem we can not use the otherwise handy built-in zip function, that creates tuples out of the pairs.\n\ndef extract_names(record, lastname_key, firstname_key):\n1    name_records = []\n2    i = -1\n3    if lastname_key in record:\n4        firstnames = record.get(firstname_key, [])\n5        for name in record.get(lastname_key):\n6            name_record = {'rid': record['id'], 'last': name, 'first': None}\n7            if name != 'et al.':\n8                i += 1\n9                if (len(firstnames) &gt; i):\n                    name_record['first'] = firstnames[i]\n            name_records.append(name_record)\n\n10        del record[lastname_key]\n11        if len(firstnames) &gt; 0:\n            del record[firstname_key]\n    \n    return name_records\n\n1\n\ncreates an empty list for the names\n\n2\n\ninitializes a counter with -1, because 0 is the index of the first element in a list and we will increase it before using it\n\n3\n\nsometimes translators are missing from the records, so we check if we should process it at all\n\n4\n\nthe first name is also missing sometimes, so we get an empty list if this is the case. If we would not add the default parameter, we would retrieve None instead which is not iterable, and raises errors when we check the length later.\n\n5\n\niterates over the last names\n\n6\n\ncreates a dictionary with None as the default first name that will be updated if there is a real first name\n\n7\n\n‘et al.’ does not have first name, so we can skip it\n\n8\n\nin every other case it increases our counter\n\n9\n\nif there is a corresponding first name by the index it updates the dictionary with that value. At the end of the check the current name will be appended to the list of names.\n\n10\n\nas we do not need it any more in the main record structure, we delete the last name\n\n11\n\nand the first name as well if it exists at all.\n\n\nWhat remained is to save the extracted data into CSV files:\ncreate_diroutput_dir = os.path.join('raw-data', 'it')\ncreate_dir(output_dir)\n\n1output_file = os.path.join(output_dir, 'it.csv')\nwith open(output_file, 'w', encoding='utf-8') as csv_file: \n2    column_names = ['id'] + list(field_names.keys())\n3    output_writer = csv.DictWriter(csv_file, fieldnames=column_names)\n4    output_writer.writeheader()\n5    output_writer.writerows(records)\n\n6column_names=['rid', 'last', 'first']\n7names = {'authors.csv': authors, 'translators.csv': translators}\n8for file_name, data in names.items():\n9    output_file = os.path.join(output_dir, file_name)\n    with open(output_file, 'w', encoding='utf-8') as csv_file:\n        output_writer = csv.DictWriter(csv_file, fieldnames=column_names)\n        output_writer.writeheader()\n        output_writer.writerows(data)\n\n1\n\nopens a CSV file raw-data/it/it.csv using the safe os.path.join function\n\n2\n\nthe column names are ‘id’ and the names we extracted from the class attributes\n\n3\n\nCSV writer’s API is similar to other file writers in Python. The csv library provides a number of functions, here we use the DictWriter that allows us to turn the list of dictionaries to CSV. It has two mandatory parameters: the file handler and the list of field names. All keys in the dictionaries should be available in the field name list, however they might be missing values in the records. E.g. if “location” is a key in the record dictionary, the field name list must contain it, but not every record should have the “location” key.\n\n4\n\nwrites out the header line to the CSV file\n\n5\n\nwrites out the individual records\n\n6\n\nrepeats the same process for authors and translators, but here we give a fixed field name list, and not extract it from the HTML.\n\n7\n\ncreates a dictionary for enabling the iteration. Its keys will be used as file names and the values are the name lists.\n\n8\n\nitems() on a dictionary iterates over its key-value pairs – we can name them anyhow.\n\n9\n\nthe rest is very similar to the first CSV writing process.\n\n\nThe full script is available in the repository as scripts/ch3a-web-scraping.py. The Intra-Belgian literary translations since 1970 (BELTRANS) project of KBR (the Belgian National Library) provides a more elaborated approach to the same problem in their Github repository – thanks to Sven Lieber for the pointer.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3b.html",
    "href": "ch3b.html",
    "title": "6  data validation",
    "section": "",
    "text": "abstract: Technical validity (XML, JSON validity check) and quality assessment.\nThe validity and quality of data are usually checked in two ways. Although there are schemas describing the structure of data and software that can be used to check whether a document complies with these schemas, in most cases these schemas are limited to describing only a general structure (for example, a MARC record contains control and data fields, the latter may contain indicators and subfields), i.e., they only affect the outermost layer of the data. It is therefore worth performing further checks – either using software available for the given format or using the Exploratory Data Analysis methodology. The simplest method is the so-called completeness check, which examines what data elements are found in the database and in what proportions.1 It is also worth examining the content of the most important data elements covered by the analyses to see how consistent they are in terms of form and content: how many different forms does the same person or geographical name appear in, or how were the dates recorded? By browsing through a frequency list of values occurring in a given data element, we can gain an understanding of the nature of the data and the harmonization tasks to be performed in the subsequent processing steps. Such a list can be coded; for example, Harald Klinke grouped the dates in the Museum of Modern Art database according to format patterns (four numbers, four numbers-four numbers, four numbers-two numbers, etc.), thus obtaining a more manageable sample list instead of many individual dates.2",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3b.html#footnotes",
    "href": "ch3b.html#footnotes",
    "title": "6  data validation",
    "section": "",
    "text": "see (Kruusmaa, Tinits, and Nemvalts 2025)↩︎\nhttps://x.com/HxxxKxxx/status/1066805548866289664↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3c.html",
    "href": "ch3c.html",
    "title": "7  preprocessing",
    "section": "",
    "text": "7.1 Creating data frame from Parquet file\nabstract: File formats, data structures, conversion, and data loss control.\nDuring preprocessing, we convert the imported files into a data structure that is more suitable for processing with standard data analysis methods (in Python, the most common is Pandas, and in R, it is the Tibble “data frame”). It may happen that we do not transform all data, but only certain records (for example, only 17th-century books from a national bibliography) or certain data elements (for example, we omit library identifiers and other administrative data elements).\nApache Parquet files can be read with the Pandas, but it needs an extra module that understands the format. We will use PyArrow module (version 23.0.0) that provides a Python API of Apache Arrow. You have to install it by\nReading a parquet file is very similar to reading CSV:\nThe result is a normal data frame, all functionalities work, such as counting the number of columns and rows:\nthe list of columns:\ndisplaying the first rows:",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>preprocessing</span>"
    ]
  },
  {
    "objectID": "ch3c.html#creating-data-frame-from-parquet-file",
    "href": "ch3c.html#creating-data-frame-from-parquet-file",
    "title": "7  preprocessing",
    "section": "",
    "text": "pip install pyarrow\n\nimport pandas as pd\n\nparquet_file = 'raw-data/lnb/natl_bibliography-2014-2023-marc.parquet'\ndf = pd.read_parquet(parquet_file, engine='auto')\n\nprint(df.shape)\n\nprint(df.columns)\n\nprint(df.head())",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>preprocessing</span>"
    ]
  },
  {
    "objectID": "ch3d.html",
    "href": "ch3d.html",
    "title": "8  data harmonisation",
    "section": "",
    "text": "abstract: normalization and data enrichment. The reproducible conversion into a data set suitable for quantitative humanities analysis.\nData maintained by others rarely fit in every respect to the specific analytical purpose for which we are preparing them. The data that are important to us must be harmonized, i.e., normalized (standardize, resolve contradictions, convert certain data types—such as particular text variables to numeric ones) and enriched (calculate derived data, such as page numbers, import data from external data sources). Below, we examine four such harmonization steps: the harmonization of dates, place names, persons, and concepts. The dates show a high degree of variation not only in the MoMe collection, but in almost every library catalog we find dates that differ from the format that is easy for programs to handle. For example, dates given in Roman numerals (“MDCCLXXX. [1780]”), in text form (“druk janvier 2016.”), according to the reign of a monarch (“Meiji 40”) or according to another calendar. Another problem is the handling of uncertain dates (e.g., “18–” and “18uu” in library catalogs both mean that the publication is from the 19th century). Due to the variety, the conversion is not trivial, but neither is deciding what to convert the dates to in the end. There are different approaches in certain areas (see, for example, archival standards or the practice of Europeana). As the latest proposal, the undate Python library1 created by the DHTech community stores the following data elements: the unchanged form of the date in the source, the calendar, the accuracy of the date, the earliest and latest normalized dates, and the duration – i.e., for the sake of consistency in retrieval, the date is always a time range. For place names, gazeeters are available for identification and, if necessary, for extra data elements required for map representation or the display of language variants. Among the most important ones are CERL Thesaurus,2 Getty Thesaurus of Geographic Names,3 and Geonames,4 which can be queried via APIs. Although these are rich databases built from many sources and thoroughly checked, practice shows that in almost every bibliographic source we will find name forms that are not recognized by these services, so these can be incorporated into our own database with some non-automated manual data refinement. The same procedure can be followed for individuals, but naturally using different services: VIAF (Virtual International Authority File),5 the CERL Thesaurus personal name database, ISNI (International Standard Name Identifier),6 Wikidata.7 It is important to note that any given database will naturally contain many more personal names than geographical names, so the hit rate is likely to be lower. The world of concepts is much more diverse than that of geographical and personal names. Although there are universal conceptual dictionaries (knowledge organization systems), there is virtually no library catalog whose records contain only the concepts of a single dictionary. Instead of specific dictionaries, we recommend using the BARTOC service (Basic Register of Thesauri, Ontologies and Classification)8 to find the dictionary that best suits your research questions. When discussing harmonization, it is essential to mention the categories of inaccurate, incomplete, subjective, and uncertain data.9 We have seen an example of inaccurate data and its handling in the case of dates. Incomplete data is when we do not know all the details, for example, not all authors of a work are listed, or there are gaps in the provenance history of an object. We can deduce some data, but it is very difficult to describe what does not exist. Subjective data refers to provenance, i.e., who made the statement in question. Such statements are often hypothetical and may even be contradictory. Finally, uncertain data is when the truthfulness of a statement is doubtful. An important part of the theories cited above is that the past is constructed, and the interpretation of sources also depends on the interpreter’s prior knowledge. Consequently, historical information systems must necessarily allow for the coexistence of contradictory interpretations, and instead of binary (true-false) logic, uncertainties could be described using probability values.10 For example, “Alexandre Dumas” (if no other information is available) could refer to either the father or the son (both writers)—the former being more likely, the value of which can be recorded in the database and used, for example, when sorting search results.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3d.html#footnotes",
    "href": "ch3d.html#footnotes",
    "title": "8  data harmonisation",
    "section": "",
    "text": "(Koeser et al. 2025)↩︎\nhttps://data.cerl.org/thesaurus/↩︎\nhttps://www.getty.edu/research/tools/vocabularies/tgn/index.html↩︎\nhttps://geonames.org↩︎\nhttps://viaf.org/↩︎\nhttps://isni.org/↩︎\nhttps://wikidata.org↩︎\nhttps://bartoc.org↩︎\n(Mariani 2023)↩︎\nThaller, Manfred, On vagueness and uncertainty in historical data = Ivory Tower blog, 2020. https://ivorytower.hypotheses.org/88.↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3e.html",
    "href": "ch3e.html",
    "title": "9  data analysis",
    "section": "",
    "text": "abstract: data analysis and data visualization with programming (Python, R) and specialized tools.\nThe most spectacular phase of the work process is data analysis.1 Here, the researcher translates his or her own research questions into the operations offered by the tools used. Two types of data are used in the analyses: on the one hand, the original or enriched data available after data harmonization (title, place of publication, language, number of pages), and on the other hand, calculated data based on their analysis (including font type and size, amount of paper used to produce the form, number of words, number of printed words per capita). Data analysis methods are not unique to bibliographic data science; they are general methods for which general and (to a lesser extent) specialized data science teaching materials in the humanities2 are available, as well as textbooks on quantitative history.3 The curriculum cannot, of course, cover every conceivable data science technique, but it should use examples to introduce the most common procedures used in the history of the discipline (the basics of statistics, time series analysis, data visualization including map representation, text analysis, network analysis).",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3e.html#footnotes",
    "href": "ch3e.html#footnotes",
    "title": "9  data analysis",
    "section": "",
    "text": "see, among other things, the literature cited in connection with the research questions.↩︎\n(Karsdorp, Kestemont, and Riddell 2021); (Klinke 2025); (Arnold and Tilton 2024); (Bátorfy 2024); and the Programming Historian site: https://programminghistorian.org.↩︎\n(Lemercier and Zalc 2019); (Feinstein and Thomas 2008); (Hudson and Ishizu 2017).↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3f.html",
    "href": "ch3f.html",
    "title": "10  dissemination",
    "section": "",
    "text": "abstract: dissemination of results. Publication of software and research data for reuse.\nThe final step in the work process is the dissemination of results, which includes traditional publication methods (papers, books, conference presentations) as well as newer approaches, such as the publication of software used in the process, the generated data, and data and software studies focusing specifically on these, blogging and microblogging, sharing presentation slides and recordings, and participating in professional organizations. I would like to emphasize the importance of special data sharing. Imagine the following scenario: a researcher has worked hard to enrich a popular data source with high research potential that is maintained by a public collection. Later, another researcher would like to use the same database for their research. If she is not familiar with the previous researcher’s work, she can start the data enrichment process from scratch. But even if the first researcher published his data enrichment, it is much more likely that the subsequent researchers will find and use the original database. To prevent this, researchers would need to return the modified data to the original data provider. Fortunately, MARC21, introduced in the 34th update in 20221 a data provenance subfield to distinguish between data recorded by the library and data recorded by the researcher (and it is available in most fields), which could be a theoretical remedy for the library’s legitimate demand to take responsibility for its own data. In the life sciences, researchers can use nanopublications to share data enrichment steps with libraries, which can then incorporate them into their catalogs without compromising their own responsibility and credibility. The second researcher can then work on the data-enriched version. In order to realize this vision, communication between the parties must be standardized, and the research community can play a coordinating role in this process.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch3f.html#footnotes",
    "href": "ch3f.html#footnotes",
    "title": "10  dissemination",
    "section": "",
    "text": "https://www.loc.gov/marc/up34bibliographic/bdapndxg.html↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "11  After the research",
    "section": "",
    "text": "The broader context. Professional communities, conferences, journals, continuing education opportunities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>After the research</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arnold, Taylor, and Lauren Tilton. 2024. Humanities\nData in R: Exploring\nNetworks, Geospatial Data,\nImages, and Text. 2nd ed. 2024.\nQuantitative Methods in the Humanities and\nSocial Sciences. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-62566-4.\n\n\nBátorfy, Attila. 2024. Adatvizualizáció. Elmélet, rendszer, módszer.\nBevezetés az adatok grafikus ábrázolásának elméletébe és gyakorlatába. Budapest: ELTE Eötvös Kiadó.\n\n\nBourdieu, Pierre. 2008. “A Conservative Revolution in\nPublishing.” Translation Studies 1 (2): 123–53. https://doi.org/10.1080/14781700802113465.\n\n\nFarkas, Farkas Gábor, János Káldos, and Péter Király. 2025. “A\nRégi Magyarországi Kiadványok „Sötét Anyaga”.” Magyar\nKönyvszemle 141 (2): 226–66. https://doi.org/10.17167/mksz.2025.2.226-266.\n\n\nFeinstein, Charles H., and Mark Thomas. 2008. Making History Count:\nA Primer in Quantitative Methods for Historians. Transferred to\ndigital printing (with corrections). Cambridge New York Melbourne Madrid\nCape Town Singapore São Paulo: Cambridge University Press.\n\n\nHeilbron, Johan. 1999. “Towards a Sociology of\nTranslation: Book Translations as\na Cultural World-System.”\nEuropean Journal of Social Theory 2 (4): 429–44. https://doi.org/10.1177/136843199002004002.\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th\nCentury? A Co-Occurrence Analysis of Early Modern\nGerman Dissertation Titles.” Synthese 203\n(2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations\nin French Libraries: The\nEMDFL Dataset.” Journal of Open\nHumanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nHudson, Pat, and Mina Ishizu. 2017. History by Numbers: An\nIntroduction to Quantitative Approaches. Second edition. London\nOxford New York New Delhi Sydney: Bloomsbury Academic.\n\n\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. 2021.\nHumanities Data Analysis: Case Studies with\nPython. Princeton Oxford: Princeton University Press.\n\n\nKirály, Péter. 2019. “Measuring Metadata\nQuality.” Doctoral dissertation, Göttingen:\nUniversity of Göttingen. https://doi.org/10.13140/RG.2.2.33177.77920.\n\n\nKirály, Péter, and András Kiséry. 2025. “‘Mór\nJókai, Alas’: The Most Successful\nHungarian Writer. A Quantitative\nAnalysis.” Patterns of Translation. https://translationpatterns.substack.com/p/mor-jokai-alas-the-most-successful.\n\n\nKirály, Péter, Tomasz Umerle, Vojtěch Malínek, Elżbieta Herden, Beata\nKoper, Giovanni Colavizza, Rindert Jagersma, et al. 2025. “Effects\nof Open Science and the Digital\nTransformation on the Bibliographical\nData Landscape.” In Library\nCatalogues as Data, edited by Paul\nGooding, Melissa Terras, and Sarah Ames, 1st ed., 19–44. Facet. https://doi.org/10.29085/9781783306602.004.\n\n\nKlinke, Harald. 2025. Cultural Data Science: An Introduction to\nR. Quantitative Methods in the Humanities and Social\nSciences. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-88130-5.\n\n\nKoeser, Rebecca Sutton, Julia Damerow, Robert Casties, and Cole\nCrawford. 2025. “Undate: Humanistic Dates for Computation:\nBecause Reality Is Frequently Inaccurate.”\nComputational Humanities Research 1: e5. https://doi.org/10.1017/chr.2025.10006.\n\n\nKruusmaa, Krister, Peeter Tinits, and Laura Nemvalts. 2025.\n“Curated Bibliographic Data:\nThe Case of the Estonian\nNational Bibliography.” Journal of\nOpen Humanities Data 11 (February): 16. https://doi.org/10.5334/johd.280.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019.\n“Bibliographic Data Science and the\nHistory of the Book (c. 1500–1800).”\nCataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.\n\n\nLemercier, Claire, and Claire Zalc. 2019. Quantitative Methods in\nthe Humanities: An Introduction. Translated by Arthur Goldhammer.\nCharlottesville: University of Virginia Press.\n\n\nMariani, Fabio. 2023. “Introducing VISU:\nVagueness, Incompleteness,\nSubjectivity, and Uncertainty in\nArt Provenance Data.” In\nProceedings of the Workshop on\nComputational Methods in the\nHumanities 2022, Vol-3602:63–84. Workshop\nProceedings. https://ceur-ws.org/Vol-3602/paper5.pdf.\n\n\nSzemes, Botond, and Kata Dobás. 2025. “A Visegrádi Országok\nDigitális Irodalmi Emlékezete : Wikipedia,\nWikidata – a Regionális Irodalomtörténet Új\nAlakzatai.” Irodalomtörténeti Közlemények 129 (2):\n191–212. https://doi.org/10.56232/itk.2025.2.04.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "ap1.html",
    "href": "ap1.html",
    "title": "Appendix A — Books",
    "section": "",
    "text": "Just as there is no teaching material specifically focused on digital book history, there are no books on this subject either. Once again, we can only recommend books on related topics.\n\nArnold, Taylor, and Lauren Tilton. 2015. Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text. Springer International Publishing. ISBN 978-3-319-20702-5 (Quantitative Methods in the Humanities and Social Sciences)\nBilbro, Rebecca, Tony Ojeda, and Benjamin Bengfort. 2018. Applied Text Analysis with Python. 1st edition. O’Reilly Media, Inc. ISBN 978-1-4919-6303-6\nGavin, Michael. 2023. Literary Mathematics: Quantitative Theory for Textual Studies. Stanford University Press. ISBN 978-1-5036-3282-0 (Stanford Text Technologies.)\nGonzales, Brighid M. 2020. Systems Librarianship: A Practical Guide for Librarians. Bloomsbury Publishing. ISBN 979-8-8818-7951-8 (Practical Guides for Librarians)\nGooding, Paul, Melissa M. Terras, and Sarah Ames, eds. 2025. Library Catalogues as Data: Research, Practice and Usage. Facet Publishing. ISBN 978-1-78330-658-9\nKlinke, Harald. 2025. Cultural Data Science: An Introduction to R. Springer Nature Switzerland. ISBN 978-3-031-88130-5. (Quantitative Methods in the Humanities and Social Sciences)\nManovich, Lev. 2020. Cultural Analytics. The MIT Press. ISBN 978-0-262-03710-5\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. 2021. Humanities Data Analysis: Case Studies with Python. Princeton University Press. ISBN 978-0-691-17236-1\nCrymble, Adam. 2021. Technology and the Historian: Transformations in the Digital Age. University of Illinois Press. ISBN 978-0-252-08569-7 (Topics in the Digital Humanities)\nJockers, Matthew L., and Rosamond Thalken. 2020. Text Analysis with R: For Students of Literature. Springer International Publishing. ISBN 978-3-030-39643-5 (Quantitative Methods in the Humanities and Social Sciences)\nNelson, Catherine. 2024. Software Engineering for Data Scientists: From Notebooks to Scalable Systems. O’Reilly. ISBN 978-1-0981-3620-8",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Books</span>"
    ]
  },
  {
    "objectID": "ap2.html",
    "href": "ap2.html",
    "title": "Appendix B — Journals",
    "section": "",
    "text": "The following journals regularly publish studies that are relevant for bibliographical data science:\n\nCode4Lib Journal\nComputational Humanities Research\nCurrent Research in Digital History\nDigital Humanities Quarterly\nDigital Scholarship in the Humanities\nInternational Journal of Digital Humanities\nInternational Journal of Humanities and Arts Computing\nJournal of Cultural Analytics\nJournal of Computing and Cultural Heritage\nJournal of Digital History\nJournal of Open Humanities Data\nOpen Library of Humanities Journal\nTransformations: A DARIAH Journal\nZeitschrift für digitale Geisteswissenschaften",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Journals</span>"
    ]
  },
  {
    "objectID": "ap3.html",
    "href": "ap3.html",
    "title": "Appendix C — Conferences",
    "section": "",
    "text": "ADHO Digital Humanities\nDARIAH Annual Conference\nComputational Humanities Research\nDigital Humanities im deutschsprachigen Raum\nDigital Humanties Benelux\nSemantic Web in Libraries\nTheory and Practice in Digital Libraries\nJoint Conference on Digital Libraries",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Conferences</span>"
    ]
  },
  {
    "objectID": "ap4.html",
    "href": "ap4.html",
    "title": "Appendix D — Data sources",
    "section": "",
    "text": "D.1 United States of America",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  },
  {
    "objectID": "ap4.html#united-states-of-america",
    "href": "ap4.html#united-states-of-america",
    "title": "Appendix D — Data sources",
    "section": "",
    "text": "Library of Congress — https://www.loc.gov/cds/products/marcDist.php. MARC21 (UTF-8 and MARC8 encoding), MARCXML formats, open access. Alternative access point: https://www.loc.gov/collections/selected-datasets/?fa=contributor:library+of+congress.+cataloging+distribution+service.\nHarvard University Library — https://library.harvard.edu/open-metadata. MARC21 format, CC0. Institution specific features are documented here\nColumbia University Library — https://library.columbia.edu/bts/clio-data.html. 10M records, MARC21 and MARCXML format, CC0.\nUniversity of Michigan Library — https://www.lib.umich.edu/open-access-bibliographic-records. 1,3M records, MARC21 and MARCXML formats, CC0.\nUniversity of Pennsylvania Libraries — https://www.library.upenn.edu/collections/digital-projects/open-data-penn-libraries. Two datasets are available:\n\nCatalog records created by Penn Libraries 572K records, MARCXML format, CC0,\nCatalog records derived from other sources, 6.5M records, MARCXML format, Open Data Commons ODC-BY, use in accordance with the OCLC community norms.\n\nYale University — https://guides.library.yale.edu/c.php?g=923429. Three datasets are available:\n\nYale-originated records: 1.47M records, MARC21 format, CC0\nWorldCat-derived records: 6.16M records, MARC21 format, ODC-BY\nOther records (MARC21), independent of Yale and WorldCat, where sharing is permitted. 404K records, MARC21 format.\n\nNational Library of Medicine (NLM) catalogue records — https://www.nlm.nih.gov/databases/download/catalog.html. 4.2 million records, NLMXML, MARCXML and MARC21 formats. NLM Terms and Conditions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  },
  {
    "objectID": "ap4.html#germany",
    "href": "ap4.html#germany",
    "title": "Appendix D — Data sources",
    "section": "D.2 Germany",
    "text": "D.2 Germany\n\nDeutsche Nationalbibliothek — https://www.dnb.de/DE/Professionell/Metadatendienste/Datenbezug/Gesamtabzuege/gesamtabzuege_node.html (note: the records are provided in utf-8 decomposed). 23.9M records, MARC21 and MARCXML format, CC0.\nBibliotheksverbundes Bayern — https://www.bib-bvb.de/web/b3kat/open-data. 27M records, MARCXML format, CC0.\nLeibniz-Informationszentrum Technik und Naturwissenschaften Universitätsbibliothek (TIB) — https://www.tib.eu/de/forschung-entwicklung/entwicklung/open-data. (no download link, use OAI-PMH instead) Dublin Core, MARC21, MARCXML, CC0.\nK10plus-Verbunddatenbank (K10plus union catalogue of Bibliotheksservice-Zentrum Baden Würtemberg (BSZ) and Gemensamer Bibliotheksverbund (GBV)) — https://swblod.bsz-bw.de/od/. 87M records, MARCXML format, CC0.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  },
  {
    "objectID": "ap4.html#others",
    "href": "ap4.html#others",
    "title": "Appendix D — Data sources",
    "section": "D.3 Others",
    "text": "D.3 Others\n\nUniversiteitsbibliotheek Gent — https://lib.ugent.be/info/exports. Weekly data dump in Aleph Sequential format. It contains some Aleph fields above the standard MARC21 fields. ODC ODbL.\nToronto Public Library — https://opendata.tplcs.ca/. 2.5 million MARC21 records, Open Data Policy\nRépertoire International des Sources Musicales — https://opac.rism.info/index.php?id=8&id=8&L=1. 800K records, MARCXML, RDF/XML, CC-BY.\nETH-Bibliothek (Swiss Federal Institute of Technology in Zurich) — http://www.library.ethz.ch/ms/Open-Data-an-der-ETH-Bibliothek/Downloads. 2.5M records, MARCXML format.\nBritish library — http://www.bl.uk/bibliographic/datafree.html#m21z3950 (no download link, use z39.50 instead after asking for permission). MARC21, usage will be strictly for non-commercial purposes.\nTalis — https://archive.org/details/talis_openlibrary_contribution. 5.5 million MARC21 records contributed by Talis to Open Library under the ODC PDDL.\nOxford Medicine Online (the catalogue of medicine books published by Oxford University Press) — https://oxfordmedicine.com/page/67/. 1790 MARC21 records.\nFennica — the Finnish National Bibliography provided by the Finnish National Library — http://data.nationallibrary.fi/download/. 1 million records, MARCXML, CC0.\nBiblioteka Narodawa (Polish National Library) — https://data.bn.org.pl/databases. 6.5 million MARC21 records.\nMagyar Nemzeti Múzeum (Hungarian National Library) — https://mnm.hu/hu/kozponti-konyvtar/nyilt-bibliografiai-adatok, 67K records, MARC21, HUNMARC, BIBFRAME, CC0\nUniversity of Amsterdam Library — https://uba.uva.nl/en/support/open-data/data-sets-and-publication-channels/data-sets-and-publication-channels.html, 2.7 million records, MARCXML, PDDL/ODC-BY. Note: the record for books are not downloadable, only other document types. One should request them via the website.\nPortugal National Library — https://opendata.bnportugal.gov.pt/downloads.htm. 1.13 million UNIMARC records in MARCXML, RDF XML, JSON, TURTLE and CSV formats. CC0\nNational Library of Latvia National bibliography (2017–2020) — https://dati.lnb.lv/. 11K MARCXML records.\nOpen datasets of the Czech National Library — https://www.en.nkp.cz/about-us/professional-activities/open-data CC0\n\nCzech National Bibliography — https://aleph.nkp.cz/data/cnb.xml.gz\nNational Authority File — https://aleph.nkp.cz/data/aut.xml.gz\nOnline Catalogue of the National Library of the Czech Republic — https://aleph.nkp.cz/data/nkc.xml.gz\nUnion Catalogue of the Czech Republic — https://aleph.nkp.cz/data/skc.xml.gz\nArticles from Czech Newspapers, Periodicals and Proceedings — https://aleph.nkp.cz/data/anl.xml.gz\nOnline Catalogue of the Slavonic Library — https://aleph.nkp.cz/data/slk.xml.gz\n\nEstonian National Bibliography — as downloadable TSV or MARC21 via OAI-PMH https://digilab.rara.ee/en/datasets/estonian-national-bibliography/ CC0\nA bibliographic dataset of the Danish Royal Library (Det Kgl. Bibliotek) including Danish monographs from approx. 1600-1900. — https://loar.kb.dk/handle/1902/49107 849K records, MARCXML, PDM 1.0\n\nThanks, Johann Rolschewski, Phú, and Hugh Paterson III for their help in collecting this list! Do you know some more data sources? Please let me know.\nThere are two more datasource worth mention, however they do not provide MARC records, but derivatives:\n\nLinked Open British National Bibliography 3.2M book records in N-Triplets and RDF/XML format, CC0 license\nLinked data of Bibliothèque nationale de France. N3, NT and RDF/XML formats, Licence Ouverte/Open Licence",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Data sources</span>"
    ]
  }
]