[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bibliographic Data Science",
    "section": "",
    "text": "Preface\nThis book is an attempt to implement the ideas described in the essay An outline of an imagined training course on bibliographic data science published in Bibliographic data blog, 2026.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Bibliographic data science is a relatively new interdisciplinary field of research that lies at the intersection of library science (or, more broadly, cultural heritage science), history and social sciences, and certain components of computer science. The objective of bibliographic data science is to establish previously hidden or possibly only suspected historical or collection trends based on data sources containing a (typically but not exclusively) large number of bibliographic records, ideally all those related to a given topic (e.g., national bibliographies), and on data science methods. Some of the field’s research questions:\n\nWhat was the spatial distribution and prosopography of 17th-century German legal dissertations? (Heßbrüggen-Walter 2025)\nWhat degree of interdisciplinarity can be observed based on the metadata of philosophical dissertations? (Heßbrüggen-Walter 2024)\nHow did the format and language of books change over time in different regions? (Lahti et al. 2019)\nWhat are the patterns of translations from a given language, how have they changed, and which languages were super-central, central, and peripheral in a given era?6\nWhat impact do publishers have on fiction?7\nWhat were the profiles of the various book collections?\nIs there a correlation between the genre and format of the book?8\nHow have genre proportions changed?9\nHow many early modern publications could have been destroyed without a trace?10\nHow can the reception of works be examined using bibliographic data?11\nWhat is the quality of cultural heritage data, and what improvement strategies can be developed?12\nHow do cultural heritage data, data structures, and standards help (or hinder) answering the above questions? What development opportunities does the research suggest for cultural heritage data standards?13\n\nAlthough digital humanities education has developed dynamically in recent years, computer-based analysis of bibliographic sources is unfortunately rarely featured, and similarly absent from library science and IT education. In my opinion, this gap could be remedied by a new informal vocational training program that would appeal to those who are interested in some of the above issues and who already have some knowledge in one of the relevant fields (e.g., library science, cultural history, literary sociology, information technology). The analysis of records based on library bibliographic standards would probably also be of interest in library training. The training may take the form of a summer university or a seminar/course jointly organized by several university departments. Participants in the training could be university students or practicing professionals.\n\n\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th Century? A Co-Occurrence Analysis of Early Modern German Dissertation Titles.” Synthese 203 (2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations in French Libraries: The EMDFL Dataset.” Journal of Open Humanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019. “Bibliographic Data Science and the History of the Book (c. 1500–1800).” Cataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch2a.html",
    "href": "ch2a.html",
    "title": "3  Scientific theories and models",
    "section": "",
    "text": "Darnton and his followers, Bourdieu, quantitative history, computational and data models of historical events – Thibodeau and Thaller",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scientific theories and models</span>"
    ]
  },
  {
    "objectID": "ch2b.html",
    "href": "ch2b.html",
    "title": "4  Cultural heritage data models",
    "section": "",
    "text": "the work-expression-manifestation-item model and its branches, ontologies, archival data models",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cultural heritage data models</span>"
    ]
  },
  {
    "objectID": "ch3a.html",
    "href": "ch3a.html",
    "title": "5  data acquisition",
    "section": "",
    "text": "5.1 Creating directories\nMain types of bibliographic data and data sources (library catalogs, citation databases, research data repositories, historical sources). Methods of data acquisition (standards, application programming interfaces, and tools), information about data structure (metadata schemas and serialization formats), terms of use.\nIn this textbook we will work with different directories: - data: the data we created - raw-data: our original data sources downloaded from data providers’ sites - plots: the output of data visualization\nTo create them we should use the os Python library that provides standard operating system functions.\nThe code of this section is available at scripts/ch3a-create-directories.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#creating-directories",
    "href": "ch3a.html#creating-directories",
    "title": "5  data acquisition",
    "section": "",
    "text": "1import os\n\n2directories = ['data', 'raw-data', 'plots']\n\n3for directory in directories:\n4    if not os.path.exists(directory):\n5        os.makedirs(directory)\n\n1\n\nimport the os library\n\n2\n\ncreate a list with three strings, the names of the directories\n\n3\n\niterate of the directories one by one. In each iteration the name of the current directory will be stored in the variable directory\n\n4\n\ncheck if the directory does not exist. If it exists the script skips the directory and takes the next one. If it doesn’t exist, it continue with the next command\n\n5\n\ncreating the directory (due to the test in the previous line only if it doesn’t exist already)",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#download-a-file",
    "href": "ch3a.html#download-a-file",
    "title": "5  data acquisition",
    "section": "5.2 Download a file",
    "text": "5.2 Download a file\nWe download a relatively small file with MARC records of the Latvian National Bibliography (2014-2023) provided by the Latvian National Library’s Open Data Portal. The files are compressed with zip, so in order to use them we should extract them,\n1import urllib.request\nimport zipfile\n\n2url = 'https://dati.lnb.lv/files/natl_bibliography-2014-2023-marc.zip'\ntarget_dir = 'raw-data'\ntarget_file = target_dir + '/lnb-natl_bibliography-2014-2023-marc.zip'\n\n3urllib.request.urlretrieve(url, target_file)\n\n4with zipfile.ZipFile(target_file, 'r') as zip_ref:\n    zip_ref.extractall(target_dir)\n\n1\n\nImport the Python libraries: urllib for the download, and zipfile for uncompressing\n\n2\n\ncreating variables\n\n3\n\ncall the download function with two arguments: the URL of the data source and the target file in our system\n\n4\n\nuncompress the zip file to our target directory\n\n\nAt the end of the process we will have a file raw-data/data/natl_bibliography-2014-2023-marc.xml, because the zip file contains a data directory. The file itself is in MARCXML format.\nThe code of this section is available at scripts/ch3a-download-a-file.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3b.html",
    "href": "ch3b.html",
    "title": "6  data validation",
    "section": "",
    "text": "Technical validity (XML, JSON validity check) and quality assessment.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3c.html",
    "href": "ch3c.html",
    "title": "7  preprocessing",
    "section": "",
    "text": "File formats, data structures, conversion, and data loss control.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>preprocessing</span>"
    ]
  },
  {
    "objectID": "ch3d.html",
    "href": "ch3d.html",
    "title": "8  data harmonisation",
    "section": "",
    "text": "normalization and data enrichment. The reproducible conversion into a data set suitable for quantitative humanities analysis.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3e.html",
    "href": "ch3e.html",
    "title": "9  data analysis",
    "section": "",
    "text": "data analysis and data visualization with programming (Python, R) and specialized tools.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3f.html",
    "href": "ch3f.html",
    "title": "10  dissemination",
    "section": "",
    "text": "dissemination of results. Publication of software and research data for reuse.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "11  After the research",
    "section": "",
    "text": "The broader context. Professional communities, conferences, journals, continuing education opportunities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>After the research</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Heßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th\nCentury? A Co-Occurrence Analysis of Early Modern\nGerman Dissertation Titles.” Synthese 203\n(2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations\nin French Libraries: The\nEMDFL Dataset.” Journal of Open\nHumanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019.\n“Bibliographic Data Science and the\nHistory of the Book (c. 1500–1800).”\nCataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.",
    "crumbs": [
      "References"
    ]
  }
]