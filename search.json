[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bibliographic Data Science",
    "section": "",
    "text": "Preface\nThis book is an attempt to implement the ideas described in the essay An outline of an imagined training course on bibliographic data science published in Bibliographic data blog, 2026.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Bibliographic data science is a relatively new interdisciplinary field of research that lies at the intersection of library science (or, more broadly, cultural heritage science), history and social sciences, and certain components of computer science. The objective of bibliographic data science is to establish previously hidden or possibly only suspected historical or collection trends based on data sources containing a (typically but not exclusively) large number of bibliographic records, ideally all those related to a given topic (e.g., national bibliographies), and on data science methods. Some of the field’s research questions:\n\nWhat was the spatial distribution and prosopography of 17th-century German legal dissertations? (Heßbrüggen-Walter 2025)\nWhat degree of interdisciplinarity can be observed based on the metadata of philosophical dissertations? (Heßbrüggen-Walter 2024)\nHow did the format and language of books change over time in different regions? (Lahti et al. 2019)\nWhat are the patterns of translations from a given language, how have they changed, and which languages were super-central, central, and peripheral in a given era?6\nWhat impact do publishers have on fiction?7\nWhat were the profiles of the various book collections?\nIs there a correlation between the genre and format of the book?8\nHow have genre proportions changed?9\nHow many early modern publications could have been destroyed without a trace?10\nHow can the reception of works be examined using bibliographic data?11\nWhat is the quality of cultural heritage data, and what improvement strategies can be developed?12\nHow do cultural heritage data, data structures, and standards help (or hinder) answering the above questions? What development opportunities does the research suggest for cultural heritage data standards?13\n\nAlthough digital humanities education has developed dynamically in recent years, computer-based analysis of bibliographic sources is unfortunately rarely featured, and similarly absent from library science and IT education. In my opinion, this gap could be remedied by a new informal vocational training program that would appeal to those who are interested in some of the above issues and who already have some knowledge in one of the relevant fields (e.g., library science, cultural history, literary sociology, information technology). The analysis of records based on library bibliographic standards would probably also be of interest in library training. The training may take the form of a summer university or a seminar/course jointly organized by several university departments. Participants in the training could be university students or practicing professionals.\n\n\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th Century? A Co-Occurrence Analysis of Early Modern German Dissertation Titles.” Synthese 203 (2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations in French Libraries: The EMDFL Dataset.” Journal of Open Humanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019. “Bibliographic Data Science and the History of the Book (c. 1500–1800).” Cataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "ch2a.html",
    "href": "ch2a.html",
    "title": "3  Scientific theories and models",
    "section": "",
    "text": "Darnton and his followers, Bourdieu, quantitative history, computational and data models of historical events – Thibodeau and Thaller",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scientific theories and models</span>"
    ]
  },
  {
    "objectID": "ch2b.html",
    "href": "ch2b.html",
    "title": "4  Cultural heritage data models",
    "section": "",
    "text": "the work-expression-manifestation-item model and its branches, ontologies, archival data models",
    "crumbs": [
      "Theoretical models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cultural heritage data models</span>"
    ]
  },
  {
    "objectID": "ch3a.html",
    "href": "ch3a.html",
    "title": "5  data acquisition",
    "section": "",
    "text": "5.1 Creating directories\nabstract: Main types of bibliographic data and data sources (library catalogs, citation databases, research data repositories, historical sources). Methods of data acquisition (standards, application programming interfaces, and tools), information about data structure (metadata schemas and serialization formats), terms of use.\nDigital data can be retrieved in three main methods. The most convenient option is for these to be available as downloadable files (e.g., as reusable research data), however this type of data sharing is relatively rare. It is more common for data sources to be accessed through some kind of application programming interface. Various applications are available for the most common interfaces (OAI-PMH, Z39.50, SRW/SRU, SPARQL), so programming is not necessarily required, but time must be set aside to study the institution-specific settings and parameters of the interfaces. Finally, it is often the case that no previous opportunity was available. At this point, we extract the data from the HTML source of the website, assuming that the typographical formatting consistently indicates certain semantic elements17 – but in this case, it is worth consulting with the website operator to see if there are any other options not documented on the site. Whichever solution you choose, make sure that the data license allows reuse. After downloading the data, the first step is to import it. Programming libraries supporting various bibliographic formats are available; for MARC21, for example, there are ones for Java, Python, Go, JavaScript, R, PHP, and other programming languages. In this textbook we will work with Python, but the code could be adapted to other programming languages.\nThere are four main types of data sources for library history research:\nIn this textbook we will work with different directories:\nThe first programming task is to create these directories. To create them we use the os Python library that provides standard operating system functions, such as makedirs that creates a directory.\nThe code of this section is available at scripts/ch3a-create-directories.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#creating-directories",
    "href": "ch3a.html#creating-directories",
    "title": "5  data acquisition",
    "section": "",
    "text": "data: the data we created\nraw-data: our original data sources downloaded from data providers’ sites\nplots: the output of data visualization\n\n\n1import os\n\n2directories = ['data', 'raw-data', 'plots']\n\n3for directory in directories:\n4    if not os.path.exists(directory):\n5        os.makedirs(directory)\n\n1\n\nimport the os library\n\n2\n\ncreate a list with three strings, the names of the directories\n\n3\n\niterate of the directories one by one. In each iteration the name of the current directory will be stored in the variable directory\n\n4\n\ncheck if the directory does not exist. If it exists the script skips the directory and takes the next one. If it doesn’t exist, it continue with the next command\n\n5\n\ncreating the directory (due to the test in the previous line only if it doesn’t exist already)",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3a.html#download-a-file",
    "href": "ch3a.html#download-a-file",
    "title": "5  data acquisition",
    "section": "5.2 Download a file",
    "text": "5.2 Download a file\nYou can find a list of downloadable library catalogues ḣere. Now download a relatively small file with MARC records of the Latvian National Bibliography (2014-2023) provided by the Open Data Portal of the National Library of Latvia. The files are compressed with zip, so in order to use them we should extract them, and because the zip file contains a data subdirectory, we rename that to lnb (after the domain name of the National Library of Latvia).\n1import urllib.request\nimport zipfile\nimport os\n\n2url = 'https://dati.lnb.lv/files/natl_bibliography-2014-2023-marc.zip'\ntarget_dir = 'raw-data'\ntarget_file = target_dir + '/lnb-natl_bibliography-2014-2023-marc.zip'\n\n3urllib.request.urlretrieve(url, target_file)\n\n4with zipfile.ZipFile(target_file, 'r') as zip_ref:\n    zip_ref.extractall(target_dir)\n\n5os.rename(target_dir + '/data', target_dir + '/lnb')\n\n1\n\nImport the Python libraries: urllib for the download, and zipfile for uncompressing\n\n2\n\ncreating variables\n\n3\n\ncall the download function with two arguments: the URL of the data source and the target file in our system\n\n4\n\nuncompress the zip file to our target directory\n\n5\n\nrename the data subdirectory (comes inside the zip file) to lnb\n\n\nAt the end of the process we will have a file raw-data/lnb/natl_bibliography-2014-2023-marc.xml, a set of MARC21 record in MARCXML serialization format, that is MARC21 in XML.\nThe code of this section is available at scripts/ch3a-download-a-file.py of the repository.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>data acquisition</span>"
    ]
  },
  {
    "objectID": "ch3b.html",
    "href": "ch3b.html",
    "title": "6  data validation",
    "section": "",
    "text": "abstract: Technical validity (XML, JSON validity check) and quality assessment.\nThe validity and quality of data are usually checked in two ways. Although there are schemas describing the structure of data and software that can be used to check whether a document complies with these schemas, in most cases these schemas are limited to describing only a general structure (for example, a MARC record contains control and data fields, the latter may contain indicators and subfields), i.e., they only affect the outermost layer of the data. It is therefore worth performing further checks – either using software available for the given format or using the Exploratory Data Analysis methodology. The simplest method is the so-called completeness check, which examines what data elements are found in the database and in what proportions.1 It is also worth examining the content of the most important data elements covered by the analyses to see how consistent they are in terms of form and content: how many different forms does the same person or geographical name appear in, or how were the dates recorded? By browsing through a frequency list of values occurring in a given data element, we can gain an understanding of the nature of the data and the harmonization tasks to be performed in the subsequent processing steps. Such a list can be coded; for example, Harald Klinke grouped the dates in the Museum of Modern Art database according to format patterns (four numbers, four numbers-four numbers, four numbers-two numbers, etc.), thus obtaining a more manageable sample list instead of many individual dates.2",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3b.html#footnotes",
    "href": "ch3b.html#footnotes",
    "title": "6  data validation",
    "section": "",
    "text": "see (Kruusmaa, Tinits, and Nemvalts 2025)↩︎\nhttps://x.com/HxxxKxxx/status/1066805548866289664↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>data validation</span>"
    ]
  },
  {
    "objectID": "ch3c.html",
    "href": "ch3c.html",
    "title": "7  preprocessing",
    "section": "",
    "text": "abstract: File formats, data structures, conversion, and data loss control.\nDuring preprocessing, we convert the imported files into a data structure that is more suitable for processing with standard data analysis methods (in Python, the most common is Pandas, and in R, it is the Tibble “data frame”). It may happen that we do not transform all data, but only certain records (for example, only 17th-century books from a national bibliography) or certain data elements (for example, we omit library identifiers and other administrative data elements).",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>preprocessing</span>"
    ]
  },
  {
    "objectID": "ch3d.html",
    "href": "ch3d.html",
    "title": "8  data harmonisation",
    "section": "",
    "text": "abstract: normalization and data enrichment. The reproducible conversion into a data set suitable for quantitative humanities analysis.\nData maintained by others rarely fit in every respect to the specific analytical purpose for which we are preparing them. The data that are important to us must be harmonized, i.e., normalized (standardize, resolve contradictions, convert certain data types—such as particular text variables to numeric ones) and enriched (calculate derived data, such as page numbers, import data from external data sources). Below, we examine four such harmonization steps: the harmonization of dates, place names, persons, and concepts. The dates show a high degree of variation not only in the MoMe collection, but in almost every library catalog we find dates that differ from the format that is easy for programs to handle. For example, dates given in Roman numerals (“MDCCLXXX. [1780]”), in text form (“druk janvier 2016.”), according to the reign of a monarch (“Meiji 40”) or according to another calendar. Another problem is the handling of uncertain dates (e.g., “18–” and “18uu” in library catalogs both mean that the publication is from the 19th century). Due to the variety, the conversion is not trivial, but neither is deciding what to convert the dates to in the end. There are different approaches in certain areas (see, for example, archival standards or the practice of Europeana). As the latest proposal, the undate Python library1 created by the DHTech community stores the following data elements: the unchanged form of the date in the source, the calendar, the accuracy of the date, the earliest and latest normalized dates, and the duration – i.e., for the sake of consistency in retrieval, the date is always a time range. For place names, gazeeters are available for identification and, if necessary, for extra data elements required for map representation or the display of language variants. Among the most important ones are CERL Thesaurus,2 Getty Thesaurus of Geographic Names,3 and Geonames,4 which can be queried via APIs. Although these are rich databases built from many sources and thoroughly checked, practice shows that in almost every bibliographic source we will find name forms that are not recognized by these services, so these can be incorporated into our own database with some non-automated manual data refinement. The same procedure can be followed for individuals, but naturally using different services: VIAF (Virtual International Authority File),5 the CERL Thesaurus personal name database, ISNI (International Standard Name Identifier),6 Wikidata.7 It is important to note that any given database will naturally contain many more personal names than geographical names, so the hit rate is likely to be lower. The world of concepts is much more diverse than that of geographical and personal names. Although there are universal conceptual dictionaries (knowledge organization systems), there is virtually no library catalog whose records contain only the concepts of a single dictionary. Instead of specific dictionaries, we recommend using the BARTOC service (Basic Register of Thesauri, Ontologies and Classification)8 to find the dictionary that best suits your research questions. When discussing harmonization, it is essential to mention the categories of inaccurate, incomplete, subjective, and uncertain data.9 We have seen an example of inaccurate data and its handling in the case of dates. Incomplete data is when we do not know all the details, for example, not all authors of a work are listed, or there are gaps in the provenance history of an object. We can deduce some data, but it is very difficult to describe what does not exist. Subjective data refers to provenance, i.e., who made the statement in question. Such statements are often hypothetical and may even be contradictory. Finally, uncertain data is when the truthfulness of a statement is doubtful. An important part of the theories cited above is that the past is constructed, and the interpretation of sources also depends on the interpreter’s prior knowledge. Consequently, historical information systems must necessarily allow for the coexistence of contradictory interpretations, and instead of binary (true-false) logic, uncertainties could be described using probability values.10 For example, “Alexandre Dumas” (if no other information is available) could refer to either the father or the son (both writers)—the former being more likely, the value of which can be recorded in the database and used, for example, when sorting search results.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3d.html#footnotes",
    "href": "ch3d.html#footnotes",
    "title": "8  data harmonisation",
    "section": "",
    "text": "(Koeser et al. 2025)↩︎\nhttps://data.cerl.org/thesaurus/↩︎\nhttps://www.getty.edu/research/tools/vocabularies/tgn/index.html↩︎\nhttps://geonames.org↩︎\nhttps://viaf.org/↩︎\nhttps://isni.org/↩︎\nhttps://wikidata.org↩︎\nhttps://bartoc.org↩︎\n(Mariani 2023)↩︎\nThaller, Manfred, On vagueness and uncertainty in historical data = Ivory Tower blog, 2020. https://ivorytower.hypotheses.org/88.↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>data harmonisation</span>"
    ]
  },
  {
    "objectID": "ch3e.html",
    "href": "ch3e.html",
    "title": "9  data analysis",
    "section": "",
    "text": "abstract: data analysis and data visualization with programming (Python, R) and specialized tools.\nThe most spectacular phase of the work process is data analysis.1 Here, the researcher translates his or her own research questions into the operations offered by the tools used. Two types of data are used in the analyses: on the one hand, the original or enriched data available after data harmonization (title, place of publication, language, number of pages), and on the other hand, calculated data based on their analysis (including font type and size, amount of paper used to produce the form, number of words, number of printed words per capita). Data analysis methods are not unique to bibliographic data science; they are general methods for which general and (to a lesser extent) specialized data science teaching materials in the humanities2 are available, as well as textbooks on quantitative history.3 The curriculum cannot, of course, cover every conceivable data science technique, but it should use examples to introduce the most common procedures used in the history of the discipline (the basics of statistics, time series analysis, data visualization including map representation, text analysis, network analysis).",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3e.html#footnotes",
    "href": "ch3e.html#footnotes",
    "title": "9  data analysis",
    "section": "",
    "text": "see, among other things, the literature cited in connection with the research questions.↩︎\n(Karsdorp, Kestemont, and Riddell 2021); (Klinke 2025); (Arnold and Tilton 2024); (Bátorfy 2024); and the Programming Historian site: https://programminghistorian.org.↩︎\n(Lemercier and Zalc 2019); (Feinstein and Thomas 2008); (Hudson and Ishizu 2017).↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>data analysis</span>"
    ]
  },
  {
    "objectID": "ch3f.html",
    "href": "ch3f.html",
    "title": "10  dissemination",
    "section": "",
    "text": "abstract: dissemination of results. Publication of software and research data for reuse.\nThe final step in the work process is the dissemination of results, which includes traditional publication methods (papers, books, conference presentations) as well as newer approaches, such as the publication of software used in the process, the generated data, and data and software studies focusing specifically on these, blogging and microblogging, sharing presentation slides and recordings, and participating in professional organizations. I would like to emphasize the importance of special data sharing. Imagine the following scenario: a researcher has worked hard to enrich a popular data source with high research potential that is maintained by a public collection. Later, another researcher would like to use the same database for their research. If she is not familiar with the previous researcher’s work, she can start the data enrichment process from scratch. But even if the first researcher published his data enrichment, it is much more likely that the subsequent researchers will find and use the original database. To prevent this, researchers would need to return the modified data to the original data provider. Fortunately, MARC21, introduced in the 34th update in 20221 a data provenance subfield to distinguish between data recorded by the library and data recorded by the researcher (and it is available in most fields), which could be a theoretical remedy for the library’s legitimate demand to take responsibility for its own data. In the life sciences, researchers can use nanopublications to share data enrichment steps with libraries, which can then incorporate them into their catalogs without compromising their own responsibility and credibility. The second researcher can then work on the data-enriched version. In order to realize this vision, communication between the parties must be standardized, and the research community can play a coordinating role in this process.",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch3f.html#footnotes",
    "href": "ch3f.html#footnotes",
    "title": "10  dissemination",
    "section": "",
    "text": "https://www.loc.gov/marc/up34bibliographic/bdapndxg.html↩︎",
    "crumbs": [
      "The data analysis workflow",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>dissemination</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "11  After the research",
    "section": "",
    "text": "The broader context. Professional communities, conferences, journals, continuing education opportunities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>After the research</span>"
    ]
  },
  {
    "objectID": "ap1.html",
    "href": "ap1.html",
    "title": "12  Books",
    "section": "",
    "text": "Just as there is no teaching material specifically focused on digital book history, there are no books on this subject either. Once again, we can only recommend books on related topics.\n\nArnold, Taylor, and Lauren Tilton. 2015. Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text. Springer International Publishing. ISBN 978-3-319-20702-5 (Quantitative Methods in the Humanities and Social Sciences)\nBilbro, Rebecca, Tony Ojeda, and Benjamin Bengfort. 2018. Applied Text Analysis with Python. 1st edition. O’Reilly Media, Inc. ISBN 978-1-4919-6303-6\nGavin, Michael. 2023. Literary Mathematics: Quantitative Theory for Textual Studies. Stanford University Press. ISBN 978-1-5036-3282-0 (Stanford Text Technologies.)\nGonzales, Brighid M. 2020. Systems Librarianship: A Practical Guide for Librarians. Bloomsbury Publishing. ISBN 979-8-8818-7951-8 (Practical Guides for Librarians)\nGooding, Paul, Melissa M. Terras, and Sarah Ames, eds. 2025. Library Catalogues as Data: Research, Practice and Usage. Facet Publishing. ISBN 978-1-78330-658-9\nKlinke, Harald. 2025. Cultural Data Science: An Introduction to R. Springer Nature Switzerland. ISBN 978-3-031-88130-5. (Quantitative Methods in the Humanities and Social Sciences)\nManovich, Lev. 2020. Cultural Analytics. The MIT Press. ISBN 978-0-262-03710-5\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. 2021. Humanities Data Analysis: Case Studies with Python. Princeton University Press. ISBN 978-0-691-17236-1\nCrymble, Adam. 2021. Technology and the Historian: Transformations in the Digital Age. University of Illinois Press. ISBN 978-0-252-08569-7 (Topics in the Digital Humanities)\nJockers, Matthew L., and Rosamond Thalken. 2020. Text Analysis with R: For Students of Literature. Springer International Publishing. ISBN 978-3-030-39643-5 (Quantitative Methods in the Humanities and Social Sciences)\nNelson, Catherine. 2024. Software Engineering for Data Scientists: From Notebooks to Scalable Systems. O’Reilly. ISBN 978-1-0981-3620-8",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Books</span>"
    ]
  },
  {
    "objectID": "ap2.html",
    "href": "ap2.html",
    "title": "13  Journals",
    "section": "",
    "text": "The following journals regularly publish studies that are relevant for bibliographical data science:\n\nCode4Lib Journal\nComputational Humanities Research\nCurrent Research in Digital History\nDigital Humanities Quarterly\nDigital Scholarship in the Humanities\nInternational Journal of Digital Humanities\nInternational Journal of Humanities and Arts Computing\nJournal of Cultural Analytics\nJournal of Computing and Cultural Heritage\nJournal of Digital History\nJournal of Open Humanities Data\nOpen Library of Humanities Journal\nTransformations: A DARIAH Journal\nZeitschrift für digitale Geisteswissenschaften",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Journals</span>"
    ]
  },
  {
    "objectID": "ap3.html",
    "href": "ap3.html",
    "title": "14  Conferences",
    "section": "",
    "text": "ADHO Digital Humanities\nDARIAH Annual Conference\nComputational Humanities Research\nDigital Humanities im deutschsprachigen Raum\nDigital Humanties Benelux\nSemantic Web in Libraries\nTheory and Practice in Digital Libraries\nJoint Conference on Digital Libraries",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conferences</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arnold, Taylor, and Lauren Tilton. 2024. Humanities\nData in R: Exploring\nNetworks, Geospatial Data,\nImages, and Text. 2nd ed. 2024.\nQuantitative Methods in the Humanities and\nSocial Sciences. Cham: Springer International\nPublishing. https://doi.org/10.1007/978-3-031-62566-4.\n\n\nBátorfy, Attila. 2024. Adatvizualizáció. Elmélet, rendszer, módszer.\nBevezetés az adatok grafikus ábrázolásának elméletébe és gyakorlatába. Budapest: ELTE Eötvös Kiadó.\n\n\nFeinstein, Charles H., and Mark Thomas. 2008. Making History Count:\nA Primer in Quantitative Methods for Historians. Transferred to\ndigital printing (with corrections). Cambridge New York Melbourne Madrid\nCape Town Singapore São Paulo: Cambridge University Press.\n\n\nHeßbrüggen-Walter, Stefan. 2024. “Interdisciplinarity in the 17th\nCentury? A Co-Occurrence Analysis of Early Modern\nGerman Dissertation Titles.” Synthese 203\n(2): 67. https://doi.org/10.1007/s11229-024-04494-2.\n\n\n———. 2025. “Early Modern Dissertations\nin French Libraries: The\nEMDFL Dataset.” Journal of Open\nHumanities Data 11 (June): 36. https://doi.org/10.5334/johd.307.\n\n\nHudson, Pat, and Mina Ishizu. 2017. History by Numbers: An\nIntroduction to Quantitative Approaches. Second edition. London\nOxford New York New Delhi Sydney: Bloomsbury Academic.\n\n\nKarsdorp, Folgert, Mike Kestemont, and Allen Riddell. 2021.\nHumanities Data Analysis: Case Studies with\nPython. Princeton Oxford: Princeton University Press.\n\n\nKlinke, Harald. 2025. Cultural Data Science: An Introduction to\nR. Quantitative Methods in the Humanities and Social\nSciences. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-88130-5.\n\n\nKoeser, Rebecca Sutton, Julia Damerow, Robert Casties, and Cole\nCrawford. 2025. “Undate: Humanistic Dates for Computation:\nBecause Reality Is Frequently Inaccurate.”\nComputational Humanities Research 1: e5. https://doi.org/10.1017/chr.2025.10006.\n\n\nKruusmaa, Krister, Peeter Tinits, and Laura Nemvalts. 2025.\n“Curated Bibliographic Data:\nThe Case of the Estonian\nNational Bibliography.” Journal of\nOpen Humanities Data 11 (February): 16. https://doi.org/10.5334/johd.280.\n\n\nLahti, Leo, Jani Marjanen, Hege Roivainen, and Mikko Tolonen. 2019.\n“Bibliographic Data Science and the\nHistory of the Book (c. 1500–1800).”\nCataloging & Classification Quarterly 57 (1): 5–23. https://doi.org/10.1080/01639374.2018.1543747.\n\n\nLemercier, Claire, and Claire Zalc. 2019. Quantitative Methods in\nthe Humanities: An Introduction. Translated by Arthur Goldhammer.\nCharlottesville: University of Virginia Press.\n\n\nMariani, Fabio. 2023. “Introducing VISU:\nVagueness, Incompleteness,\nSubjectivity, and Uncertainty in\nArt Provenance Data.” In\nProceedings of the Workshop on\nComputational Methods in the\nHumanities 2022, Vol-3602:63–84. Workshop\nProceedings. https://ceur-ws.org/Vol-3602/paper5.pdf.",
    "crumbs": [
      "References"
    ]
  }
]